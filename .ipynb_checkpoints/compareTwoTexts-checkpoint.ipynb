{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b86117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88454644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/genData/text_sentences_words.csv') # this file has the texts with punctuation!\n",
    "# remove [stuff] in between square brackets - should already be gone now\n",
    "def remove_bracket(text):\n",
    "    return re.sub('(\\[[^w]*\\]\\s)', '',text)\n",
    "df['text'] = df['text'].apply(remove_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f0cb495d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filepath</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>word_set</th>\n",
       "      <th>num_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Let me first of all say how grateful I am for ...</td>\n",
       "      <td>./Data/speeches/2012-07-21-AuroraShooting.txt</td>\n",
       "      <td>2012-07-21</td>\n",
       "      <td>oba</td>\n",
       "      <td>[\"Let me first of all say how grateful I am fo...</td>\n",
       "      <td>['Let', 'me', 'first', 'of', 'all', 'say', 'ho...</td>\n",
       "      <td>45</td>\n",
       "      <td>887</td>\n",
       "      <td>{'basis', 'have', 'others.', 'hopes', 'makes',...</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "37  Let me first of all say how grateful I am for ...   \n",
       "\n",
       "                                    filepath        date source  \\\n",
       "37  ./Data/speeches/2012-07-21-AuroraShooting.txt  2012-07-21    oba   \n",
       "\n",
       "                                            sentences  \\\n",
       "37  [\"Let me first of all say how grateful I am fo...   \n",
       "\n",
       "                                                words  num_sents  num_words  \\\n",
       "37  ['Let', 'me', 'first', 'of', 'all', 'say', 'ho...         45        887   \n",
       "\n",
       "                                             word_set  num_unique_words  \n",
       "37  {'basis', 'have', 'others.', 'hopes', 'makes',...               369  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date1='2012-07-21'\n",
    "source1='oba'\n",
    "text1=df.query('date==@date1 and source==@source1')\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11f29675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filepath</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>word_set</th>\n",
       "      <th>num_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Thousands of secret military documents were re...</td>\n",
       "      <td>./Data/WSJ/2010-07-26-Wikileaks.txt</td>\n",
       "      <td>2010-07-26</td>\n",
       "      <td>wsj</td>\n",
       "      <td>['Thousands of secret military documents were ...</td>\n",
       "      <td>['Thousands', 'of', 'secret', 'military', 'doc...</td>\n",
       "      <td>39</td>\n",
       "      <td>1043</td>\n",
       "      <td>{'have', 'led', 'charged', 'insurgent', 'sugge...</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "264  Thousands of secret military documents were re...   \n",
       "\n",
       "                           filepath        date source  \\\n",
       "264  ./Data/WSJ/2010-07-26-Wikileaks.txt  2010-07-26    wsj   \n",
       "\n",
       "                                             sentences  \\\n",
       "264  ['Thousands of secret military documents were ...   \n",
       "\n",
       "                                                 words  num_sents  num_words  \\\n",
       "264  ['Thousands', 'of', 'secret', 'military', 'doc...         39       1043   \n",
       "\n",
       "                                              word_set  num_unique_words  \n",
       "264  {'have', 'led', 'charged', 'insurgent', 'sugge...               474  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date2='2010-07-26'\n",
    "source2='wsj'\n",
    "text2=df.query('date==@date2 and source==@source2')\n",
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ea14a",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519705d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full list of processors.... only need tokenize and pos for now\n",
    "#============================\n",
    "#| Processor    | Package   |\n",
    "#----------------------------\n",
    "#| tokenize     | combined  |\n",
    "#| pos          | combined  |\n",
    "#| lemma        | combined  |\n",
    "#| depparse     | combined  |\n",
    "#| sentiment    | sstplus   |\n",
    "#| constituency | wsj       |\n",
    "#| ner          | ontonotes |\n",
    "#============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b9d5609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0a5b78b92746ec924c4061c4d749e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 11:06:48 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-03-01 11:06:49 INFO: File exists: /home/muddy/stanza_resources/en/default.zip\n",
      "2023-03-01 11:06:53 INFO: Finished downloading models and saved to /home/muddy/stanza_resources.\n",
      "2023-03-01 11:06:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa95162bc21b4881af6ea7595f4435f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 11:06:54 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-03-01 11:06:54 INFO: Use device: cpu\n",
      "2023-03-01 11:06:54 INFO: Loading: tokenize\n",
      "2023-03-01 11:06:54 INFO: Loading: pos\n",
      "2023-03-01 11:06:54 INFO: Loading: lemma\n",
      "2023-03-01 11:06:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp_st = stanza.Pipeline('en', processors='tokenize, lemma, pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60457790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_count(items):\n",
    "    '''items: list of objects to count\n",
    "       returns: dictionary with key:item, value: cumulative count of each item'''\n",
    "    counts = defaultdict(int)\n",
    "    cumulative_counts = {}\n",
    "    cumulative_count = 0\n",
    "    \n",
    "    for item in items:\n",
    "        counts[item] += 1\n",
    "        cumulative_count += 1\n",
    "        cumulative_counts[item] = cumulative_count\n",
    "        \n",
    "    return(cumulative_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "39db4b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'dict_values' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [86], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     pos_prop \u001b[38;5;241m=\u001b[39m pos_counts\u001b[38;5;241m.\u001b[39mvalues()\u001b[38;5;241m/\u001b[39msum1\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pos_prop)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mget_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m37\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [86], line 11\u001b[0m, in \u001b[0;36mget_pos\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m pos_counts \u001b[38;5;241m=\u001b[39m cumulative_count(list_of_pos) \u001b[38;5;66;03m# get dictionary with key: pos, value: count of that POS in speech\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sum1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(pos_counts\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m---> 11\u001b[0m pos_prop \u001b[38;5;241m=\u001b[39m \u001b[43mpos_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43msum1\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos_prop)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'dict_values' and 'int'"
     ]
    }
   ],
   "source": [
    "# OK Lets put it all together and loop over all speeches and count up the POS\n",
    "# The nlp(text) uses a lot of gpu memory, causes errors sometimes, may need to restart notebook to freshen things up\n",
    "pos_dicts = []\n",
    "def get_pos(text):\n",
    "    doc = nlp_st(text) # Run stanza on each speech\n",
    "    mat_of_pos = [[word.pos for word in sentence.words] for sentence in doc.sentences] # matrix of POS for each sentence\n",
    "    # How to flatten a list = [item for sublist in list_of_lists for item in sublist]\n",
    "    list_of_pos = [pos for sentence in mat_of_pos for pos in sentence] # flatten matrix into one list of all pos\n",
    "    pos_counts = cumulative_count(list_of_pos) # get dictionary with key: pos, value: count of that POS in speech\n",
    "    sum1=sum(pos_counts.values())\n",
    "    pos_prop = pos_counts.values()/sum1\n",
    "    print(pos_prop)\n",
    "get_pos(text1.loc[37, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75cd7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_st(text1.loc[37, 'text']) # Run stanza on each speech\n",
    "mat_of_pos = [[word.pos for word in sentence.words] for sentence in doc.sentences] # matrix of POS for each sentence\n",
    "# How to flatten a list = [item for sublist in list_of_lists for item in sublist]\n",
    "list_of_pos = [pos for sentence in mat_of_pos for pos in sentence] # flatten matrix into one list of all pos\n",
    "pos_counts = cumulative_count(list_of_pos) # get dictionary with key: pos, value: count of that POS in speech\n",
    "sum1=sum(pos_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pick up here later and fix the previous two cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
