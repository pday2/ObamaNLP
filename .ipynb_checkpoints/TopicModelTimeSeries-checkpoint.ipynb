{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3812d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import csv\n",
    "import regex as re\n",
    "import os\n",
    "import spacy\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e072b7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './DataUCSB/'\n",
    "list_of_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        list_of_files.append(os.path.join(root,file))\n",
    "\n",
    "len(list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cf19959",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './DataUCSB/address-before-joint-session-the-congress-the-state-the-union-16.csv'\n",
    "speeches = []\n",
    "for file in list_of_files:\n",
    "    with open(filepath, 'r') as read_obj: # read csv file as a list of lists\n",
    "      csv_reader = csv.reader(read_obj) # pass the file object to reader() to get the reader object\n",
    "      speechList = sum(list(csv_reader), []) # Pass reader object to list() to get a list of lists (matrix)\n",
    "                                            # sum(list, []) flattens 2D matrix into a vector\n",
    "      speech = ''.join(speechList)\n",
    "    speeches.append(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a90a725",
   "metadata": {},
   "source": [
    "<A HREF=\"https://regexr.com/\">RegExr regex tester</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cf6113",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     15\u001b[0m    temp \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m    summaries\u001b[38;5;241m.\u001b[39mappend(preprocess(\u001b[43mtemp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create a dictionary representation of the documents.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m Dictionary(summaries)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#tokenize, remove stopwords, non-alphabetic words, lowercase\n",
    "def preprocess(textstring):\n",
    "   stops =  set(stopwords.words('english'))\n",
    "   tokens = word_tokenize(textstring)\n",
    "   return [token.lower() for token in tokens if token.isalpha() and token not in stops]\n",
    "\n",
    "# This is a sample path of your downloaded data set. This is currently set to a windows based path . \n",
    "# Please update it to your actual download path regradless of your choice of operating system \n",
    "\n",
    "data_path = os.path.join(os.path.dirname(os.path.abspath(filepath)),data)\n",
    "\n",
    "summaries = []\n",
    "for line in open(data_path, encoding=\"utf-8\"):\n",
    "   temp = line.split(\"\\t\")\n",
    "   summaries.append(preprocess(temp[6]))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "\n",
    "dictionary = Dictionary(summaries)\n",
    "\n",
    "# Filter infrequent or too frequent words.\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(summary) for summary in summaries]\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "#Train the topic model\n",
    "\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\n",
    "top_topics = list(model.top_topics(corpus))\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23745b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = [nlp(speech) for speech in speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a2c6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tags to remove\n",
    "extags = ['PRON','CCONJ','PUNCT','PART','DET','ADP','NUM','SYM','SPACE']\n",
    "tokens = [ (token.lemma_.lower() for token in speech if token.pos_ not in extags and not token.is_stop and token.is_alpha) for speech in doc ]\n",
    "\n",
    "dictionary = Dictionary(tokens)\n",
    "#dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "#Convert token counts into bag of words (BoW) corpus\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ba838b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cca3b2e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make a index to word dictionary.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mdictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# This is only to \"load\" the dictionary.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m id2word \u001b[38;5;241m=\u001b[39m dictionary\u001b[38;5;241m.\u001b[39mid2token\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Train the topic model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gensim/corpora/dictionary.py:107\u001b[0m, in \u001b[0;36mDictionary.__getitem__\u001b[0;34m(self, tokenid)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2token) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# the word->id mapping has changed (presumably via add_documents);\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# recompute id->word accordingly\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2token \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mrevdict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid2token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenid\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Make a index to word dictionary.\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "#Train the topic model\n",
    "\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\n",
    "top_topics = list(model.top_topics(corpus))\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working\n",
    "\n",
    "#Construct and train unsupervised LDA model + Determine optimal amount of topics\n",
    "umtopics, umscore = [], []\n",
    "cvtopics, cvscore = [], []\n",
    "#Compute coherence score using C_umass:\n",
    "for i in range(1,20,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=dict, iterations=50, num_topics=i, workers=4, passes=10, random_state=47)\n",
    "    #LdamultiCore uses multiple cores to speed up model training, use with caution if you have a weaker PC! (Find your max number of cores with ctrl+shift+esc, under CPU)\n",
    "    cm = CoherenceModel(model=model, corpus=corpus, dictionary=dict, coherence='u_mass')\n",
    "\n",
    "    umtopics.append(i)\n",
    "    umscore.append(cm.get_coherence())\n",
    "#Compute coherence score using C_v: \n",
    "for i in range (1,20,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=dict, iterations=10, num_topics=i, workers = 4, passes=10, random_state=47)\n",
    "    cm = CoherenceModel(model=model, texts = tokens, corpus=corpus, dictionary=dict, coherence='c_v')\n",
    "\n",
    "    cvtopics.append(i)\n",
    "    cvscore.append(cm.get_coherence())\n",
    "#The difference in coherence score measures is the method in which the text is segmented and probability is calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06df3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
