{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e16f42",
   "metadata": {},
   "source": [
    "# A look at some of the readability score metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ab384",
   "metadata": {},
   "source": [
    "<A HREF=\"https://en.wikipedia.org/wiki/Readability\">Wikipedia - Readability</A><BR>\n",
    "<A HREF=\"https://www.geeksforgeeks.org/readability-index-pythonnlp/\">GeeksforGeeks Readability - Index in Python</A><BR>\n",
    "<A HREF=\"https://pypi.org/project/readability/\">Readability python package</A><BR>\n",
    "    <A HREF=\"https://pypi.org/project/textstat/\">Textstat python package</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cad4362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f4d037abd60>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "import en_core_web_md\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from collections import defaultdict\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import readability\n",
    "import pprint # pretty print for easy printing of ordered dictionary\n",
    "# import spacy # Needed for GeeksforGeeks code which is a mess\n",
    "from textstat.textstat import textstatistics\n",
    "spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b92c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>...</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>depth</th>\n",
       "      <th>TBsubjectivity</th>\n",
       "      <th>TBpolarity</th>\n",
       "      <th>words_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-06-04</td>\n",
       "      <td>nyt</td>\n",
       "      <td>0.064458</td>\n",
       "      <td>0.065088</td>\n",
       "      <td>0.06240</td>\n",
       "      <td>0.064962</td>\n",
       "      <td>0.064416</td>\n",
       "      <td>0.063408</td>\n",
       "      <td>0.055598</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.046218</td>\n",
       "      <td>0.130252</td>\n",
       "      <td>47</td>\n",
       "      <td>1459</td>\n",
       "      <td>620</td>\n",
       "      <td>7.276596</td>\n",
       "      <td>0.406976</td>\n",
       "      <td>0.135880</td>\n",
       "      <td>31.042553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-06-04</td>\n",
       "      <td>oba</td>\n",
       "      <td>0.064649</td>\n",
       "      <td>0.064670</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.064398</td>\n",
       "      <td>0.064471</td>\n",
       "      <td>0.064638</td>\n",
       "      <td>0.054967</td>\n",
       "      <td>0.064503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.048739</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>217</td>\n",
       "      <td>5856</td>\n",
       "      <td>939</td>\n",
       "      <td>5.986175</td>\n",
       "      <td>0.445383</td>\n",
       "      <td>0.167408</td>\n",
       "      <td>26.986175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date source       ADJ       ADP      ADV       AUX     CCONJ  \\\n",
       "0 2008-06-04    nyt  0.064458  0.065088  0.06240  0.064962  0.064416   \n",
       "1 2008-06-04    oba  0.064649  0.064670  0.06444  0.064398  0.064471   \n",
       "\n",
       "        DET      INTJ      NOUN  ...   sadness  surprise     trust  num_sents  \\\n",
       "0  0.063408  0.055598  0.064920  ...  0.075630  0.046218  0.130252         47   \n",
       "1  0.064638  0.054967  0.064503  ...  0.042017  0.048739  0.159664        217   \n",
       "\n",
       "   num_words  num_unique_words     depth  TBsubjectivity TBpolarity  \\\n",
       "0       1459               620  7.276596        0.406976   0.135880   \n",
       "1       5856               939  5.986175        0.445383   0.167408   \n",
       "\n",
       "   words_per_sentence  \n",
       "0           31.042553  \n",
       "1           26.986175  \n",
       "\n",
       "[2 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidy_data = pd.read_csv('tidy_data.csv')\n",
    "tidy_data['date'] = pd.to_datetime(tidy_data['date'], format='%Y-%m-%d')\n",
    "tidy_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c3ca92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-07-28</td>\n",
       "      <td>oba</td>\n",
       "      <td>On behalf of the great state of Illinois, cros...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-07-28</td>\n",
       "      <td>oba</td>\n",
       "      <td>Tonight is a particular honor for me because, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date source                                           sentence  depth\n",
       "0 2004-07-28    oba  On behalf of the great state of Illinois, cros...      8\n",
       "1 2004-07-28    oba  Tonight is a particular honor for me because, ...      5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.read_csv('sentence_depth.csv')\n",
    "sentences['date'] = pd.to_datetime(sentences['date'], format='%Y-%m-%d')\n",
    "sentences.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0daaa62",
   "metadata": {},
   "source": [
    "### First, using the readability package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2f31ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('readability grades',\n",
      "              OrderedDict([('Kincaid', 11.765714285714285),\n",
      "                           ('ARI', 15.447142857142858),\n",
      "                           ('Coleman-Liau', 11.701783714285718),\n",
      "                           ('FleschReadingEase', 60.57928571428573),\n",
      "                           ('GunningFogIndex', 16.914285714285715),\n",
      "                           ('LIX', 60.142857142857146),\n",
      "                           ('SMOGIndex', 13.954451150103322),\n",
      "                           ('RIX', 9.0),\n",
      "                           ('DaleChallIndex', 11.228514285714287)])),\n",
      "             ('sentence info',\n",
      "              OrderedDict([('characters_per_word', 4.857142857142857),\n",
      "                           ('syll_per_word', 1.3928571428571428),\n",
      "                           ('words_per_sentence', 28.0),\n",
      "                           ('sentences_per_paragraph', 1.0),\n",
      "                           ('type_token_ratio', 0.8214285714285714),\n",
      "                           ('characters', 136),\n",
      "                           ('syllables', 39),\n",
      "                           ('words', 28),\n",
      "                           ('wordtypes', 23),\n",
      "                           ('sentences', 1),\n",
      "                           ('paragraphs', 1),\n",
      "                           ('long_words', 9),\n",
      "                           ('complex_words', 4),\n",
      "                           ('complex_words_dc', 11)])),\n",
      "             ('word usage',\n",
      "              OrderedDict([('tobeverb', 0),\n",
      "                           ('auxverb', 0),\n",
      "                           ('conjunction', 0),\n",
      "                           ('pronoun', 3),\n",
      "                           ('preposition', 5),\n",
      "                           ('nominalization', 1)])),\n",
      "             ('sentence beginnings',\n",
      "              OrderedDict([('pronoun', 0),\n",
      "                           ('interrogative', 0),\n",
      "                           ('article', 0),\n",
      "                           ('subordination', 0),\n",
      "                           ('conjunction', 0),\n",
      "                           ('preposition', 1)]))])\n"
     ]
    }
   ],
   "source": [
    "# Text should be encoded with UTF-8, one sentence per line, tokens space-separated.\n",
    "results = readability.getmeasures(sentences['sentence'][0], lang='en')\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "602ffe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Kincaid', 11.765714285714285),\n",
      "             ('ARI', 15.447142857142858),\n",
      "             ('Coleman-Liau', 11.701783714285718),\n",
      "             ('FleschReadingEase', 60.57928571428573),\n",
      "             ('GunningFogIndex', 16.914285714285715),\n",
      "             ('LIX', 60.142857142857146),\n",
      "             ('SMOGIndex', 13.954451150103322),\n",
      "             ('RIX', 9.0),\n",
      "             ('DaleChallIndex', 11.228514285714287)])\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(results['readability grades'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd44c21",
   "metadata": {},
   "source": [
    "### Now, try GeeksforGeeks code. OK, their code is a mess and needs some work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c812e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the text into sentences, using\n",
    "# Spacy's sentence segmentation which can\n",
    "# be found at https://spacy.io/usage/spacy-101\n",
    "def break_sentences(text):\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    doc = nlp(text)\n",
    "    return list(doc.sents)\n",
    " \n",
    "# Returns Number of Words in the text\n",
    "def word_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words\n",
    " \n",
    "# Returns the number of sentences in the text\n",
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    return len(sentences)\n",
    " \n",
    "# Returns average sentence length\n",
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    " \n",
    "# Textstat is a python package, to calculate statistics from\n",
    "# text to determine readability,\n",
    "# complexity and grade level of a particular corpus.\n",
    "# Package can be found at https://pypi.python.org/pypi/textstat\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    " \n",
    "# Returns the average number of syllables per\n",
    "# word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return _legacy_round(ASPW, 1)\n",
    " \n",
    "# Return total Difficult Words in a text\n",
    "def difficult_words(text):\n",
    "     \n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    doc = nlp(text)\n",
    "    # Find all words in the text\n",
    "    words = []\n",
    "    sentences = break_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        words += [str(token) for token in sentence]\n",
    " \n",
    "    # difficult words are those with syllables >= 2\n",
    "    # easy_word_set is provide by Textstat as\n",
    "    # a list of common words\n",
    "    diff_words_set = set()\n",
    "     \n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    " \n",
    "    return len(diff_words_set)\n",
    " \n",
    "# A word is polysyllablic if it has more than 3 syllables\n",
    "# this functions returns the number of all such words\n",
    "# present in the text\n",
    "def poly_syllable_count(text):\n",
    "    count = 0\n",
    "    words = []\n",
    "    sentences = break_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        words += [token for token in sentence]\n",
    "     \n",
    " \n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if syllable_count >= 3:\n",
    "            count += 1\n",
    "    return count\n",
    " \n",
    " \n",
    "def flesch_reading_ease(text):\n",
    "    \"\"\"\n",
    "        Implements Flesch Formula:\n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words\n",
    "                divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables\n",
    "                divided by number of words)\n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) -\\\n",
    "          float(84.6 * avg_syllables_per_word(text))\n",
    "    return _legacy_round(FRE, 2)\n",
    " \n",
    " \n",
    "def gunning_fog(text):\n",
    "    per_diff_words = (difficult_words(text) / word_count(text) * 100) + 5\n",
    "    grade = 0.4 * (avg_sentence_length(text) + per_diff_words)\n",
    "    return grade\n",
    " \n",
    " \n",
    "def smog_index(text):\n",
    "    \"\"\"\n",
    "        Implements SMOG Formula / Grading\n",
    "        SMOG grading = 3 + ?polysyllable count.\n",
    "        Here,\n",
    "           polysyllable count = number of words of more\n",
    "          than two syllables in a sample of 30 sentences.\n",
    "    \"\"\"\n",
    " \n",
    "    if sentence_count(text) >= 3:\n",
    "        poly_syllab = poly_syllable_count(text)\n",
    "        SMOG = (1.043 * (30*(poly_syllab / sentence_count(text)))**0.5) \\\n",
    "                + 3.1291\n",
    "        return legacy_round(SMOG, 1)\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    " \n",
    "def dale_chall_readability_score(text):\n",
    "    \"\"\"\n",
    "        Implements Dale Challe Formula:\n",
    "        Raw score = 0.1579*(PDW) + 0.0496*(ASL) + 3.6365\n",
    "        Here,\n",
    "            PDW = Percentage of difficult words.\n",
    "            ASL = Average sentence length\n",
    "    \"\"\"\n",
    "    words = word_count(text)\n",
    "    # Number of words not termed as difficult words\n",
    "    count = word_count - difficult_words(text)\n",
    "    if words > 0:\n",
    " \n",
    "        # Percentage of words not on difficult word list\n",
    " \n",
    "        per = float(count) / float(words) * 100\n",
    "     \n",
    "    # diff_words stores percentage of difficult words\n",
    "    diff_words = 100 - per\n",
    " \n",
    "    raw_score = (0.1579 * diff_words) + \\\n",
    "                (0.0496 * avg_sentence_length(text))\n",
    "     \n",
    "    # If Percentage of Difficult Words is greater than 5 %, then;\n",
    "    # Adjusted Score = Raw Score + 3.6365,\n",
    "    # otherwise Adjusted Score = Raw Score\n",
    " \n",
    "    if diff_words > 5:      \n",
    " \n",
    "        raw_score += 3.6365\n",
    "         \n",
    "    return legacy_round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5a4257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difficult_words(sentences['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fce6e0be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.span.Span' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpoly_syllable_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [28], line 75\u001b[0m, in \u001b[0;36mpoly_syllable_count\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     73\u001b[0m sentences \u001b[38;5;241m=\u001b[39m break_sentences(text)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 75\u001b[0m     words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m     79\u001b[0m     syllable_count \u001b[38;5;241m=\u001b[39m syllables_count(word)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.span.Span' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "poly_syllable_count(sentences['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5929b0cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_legacy_round' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mflesch_reading_ease\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [35], line 96\u001b[0m, in \u001b[0;36mflesch_reading_ease\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflesch_reading_ease\u001b[39m(text):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m        Implements Flesch Formula:\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m                divided by number of words)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     FRE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m206.835\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m1.015\u001b[39m \u001b[38;5;241m*\u001b[39m avg_sentence_length(text)) \u001b[38;5;241m-\u001b[39m\\\n\u001b[0;32m---> 96\u001b[0m           \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m84.6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mavg_syllables_per_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_round(FRE, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn [35], line 42\u001b[0m, in \u001b[0;36mavg_syllables_per_word\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     40\u001b[0m words \u001b[38;5;241m=\u001b[39m word_count(text)\n\u001b[1;32m     41\u001b[0m ASPW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(syllable) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(words)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_round\u001b[49m(ASPW, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_legacy_round' is not defined"
     ]
    }
   ],
   "source": [
    "flesch_reading_ease(sentences['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85d56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
