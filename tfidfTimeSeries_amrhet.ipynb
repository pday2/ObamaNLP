{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2e6945",
   "metadata": {},
   "source": [
    "### The idea here is to calculate tf-idf for all the words in Obama's speeches, then take the 100 with the highest average or 100 with highest max value, maybe cluster them, then plot them as a time series with their tf-idf values for all the speeches and see if any travel together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a6c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 11:05:35.073009: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 11:05:35.596907: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-24 11:05:36.539538: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 11:05:36.539678: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 11:05:36.539686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-24 11:05:37.426374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-24 11:05:37.426705: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-24 11:05:37.426725: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (muddy-HP-ProDesk-600-G3-SFF): /proc/driver/nvidia/version does not exist\n",
      "[nltk_data] Downloading package punkt to /home/muddy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/muddy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import en_core_web_md\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pprint # pretty print for easy printing of ordered dictionary\n",
    "import spacy\n",
    "spacy.load('en_core_web_md')\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download ('wordnet')\n",
    "# Time series clustering\n",
    "import math\n",
    "# https://tslearn.readthedocs.io/en/stable/\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4b2f8",
   "metadata": {},
   "source": [
    "### Load up some speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f819db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['./Data/']\n",
    "\n",
    "speeches = []\n",
    "\n",
    "for path in paths:\n",
    "    list_of_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                list_of_files.append(os.path.join(root,file))\n",
    "   \n",
    "    for file in list_of_files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        speeches.append(text)\n",
    "\n",
    "#clean out goofy unicode  space characters \n",
    "speeches = [unicodedata.normalize(\"NFKD\", speech) for speech in speeches if len(speech)>0 ]\n",
    "#clean out xa0 space characters\n",
    "[speech.replace(u'\\xa0', '') for speech in speeches]; # ; supresses output\n",
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "speeches = [remove_bracket(speech) for speech in speeches]\n",
    "\n",
    "# lemmatize text with spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"parser\", \"ner\"])\n",
    "for i, doc in enumerate(nlp.pipe(speeches)):\n",
    "    words_lemmas_list = [token.lemma_ for token in doc]\n",
    "    speeches[i]= ' '.join(words_lemmas_list)\n",
    "    \n",
    "# Remove punctuation - only care about words here\n",
    "speeches = [re.sub(r'[^\\w\\s]', '', speech) for speech in speeches]\n",
    "\n",
    "# remove numbers\n",
    "speeches = [re.sub(r'\\d', '', speech) for speech in speeches]\n",
    "\n",
    "# Clean up whitespace\n",
    "speeches = [re.sub('[\\s+]', ' ', speech) for speech in speeches]\n",
    "\n",
    "df = pd.DataFrame({'filepath' : list_of_files,\n",
    "                   'text' : speeches})\n",
    "datetitle = pd.read_csv('datetitle.csv')\n",
    "#datetitle.url = [file.replace('Data/', './Data/') for file in datetitle.url]\n",
    "datetitle.date = pd.to_datetime(datetitle.date, format='%Y-%m-%d')\n",
    "datetitle = datetitle.drop('title', axis=1)\n",
    "datetitle = datetitle.rename(columns={'url': 'filepath'})\n",
    "df = pd.merge(df, datetitle, how='inner', on='filepath')\n",
    "df = df.sort_values(by='date', ignore_index=True)\n",
    "df = df[['date', 'filepath', 'text']]\n",
    "df['source'] = 'oba'\n",
    "df.set_index('date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca32a36",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">sk-learn TfidfVectorizer</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3cf1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want to remove stop words before creating contingency table\n",
    "kaggle_file = open(\"./word_lists/kaggle_stopwords.txt\", \"r\")\n",
    "kaggle_data = kaggle_file.read()\n",
    "kaggle_list = [word for word in kaggle_data.split('\\n')]\n",
    "kaggle_file.close()\n",
    "my_list = ['thats', 'just', 'im', 'did', 'thing', 'mr', 'al', 'thank', 'okay', 'thank','thanks', \n",
    "           'question', 'joshua', 'president', 'obama', 'Ã¢', u'\\x99s', u'\\x99t', u'\\x99ve', u'\\x99m',u'\\x99re', '\\x99']\n",
    "stop_list = list(set(kaggle_list) | set(my_list))\n",
    "stop_words_kag = ENGLISH_STOP_WORDS.union(stop_list)\n",
    "\n",
    "#for i in range(len(text_df)):\n",
    "#        text_df['text'].iloc[i] = ' '.join([word for word in text_df['text'].iloc[i].split() if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da6ab3",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca8e1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = list(set(stop_words) | set(stop_list))\n",
    "# Run TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_df=0.9)\n",
    "tfidf_matrix = vectorizer.fit_transform(text_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68c33f",
   "metadata": {},
   "source": [
    "### Resulting tf-idf dataframe: rows are speeches, columns are words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87528a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "# df is roughly (101, 8113) unless I add more text cleanup, like removing stop words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb85948",
   "metadata": {},
   "source": [
    "### Find optimal number of clusters. First cell uses sklearn, second uses tslearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2a542",
   "metadata": {},
   "source": [
    "### Comment out unwanted column selection methods. 'max' selects the columns with the highest maximum tf-idf value. 'mean' selects the columns with the highest mean tf-idf values. 'non-zero' selects the columns with the most number of non-zero tf-idf values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e081389",
   "metadata": {},
   "source": [
    "### Maybe the optimal number of words to look at isn't constant across the three word choosing methods???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max\n",
    "df.max().sort_values(ascending=False)[0:200].plot(use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ccc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "df.mean().sort_values(ascending=False)[0:200].plot(use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-zero\n",
    "df.astype(bool).sum(axis=0).sort_values(ascending=False)[0:2000].plot(use_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f65c1b",
   "metadata": {},
   "source": [
    "### Choose optimal number of clusters using scree plot, first for sklearn's kmeans algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sklearn k-means algo - # max - ??? , mean - 12, nonzero - 12?\n",
    "# Choose best by max value\n",
    "num_words = 100\n",
    "columns = df.median().sort_values(ascending=False)[0:num_words].index\n",
    "# Choose best by mean value\n",
    "#num_words = 100\n",
    "#columns = df.mean().sort_values(ascending=False)[0:num_words].index\n",
    "# Choose by number of non-zero values for each word\n",
    "#num_words = 150\n",
    "#columns = df.astype(bool).sum(axis=0).sort_values(ascending=False)[0:num_words].index\n",
    "\n",
    "subset = df[columns]\n",
    "# Find number of clusters for k-means clustering\n",
    "#initialize kmeans parameters\n",
    "kmeans_kwargs = {\n",
    "\"init\": \"random\",\n",
    "\"n_init\": 10,\n",
    "\"random_state\": 33,\n",
    "}\n",
    "\n",
    "#create list to hold SSE values for each k\n",
    "sse = []\n",
    "for k in range(1, 20):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(subset.T)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "#visualize results\n",
    "plt.plot(range(1, 20), sse)\n",
    "plt.xticks(range(1, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478740d",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.TimeSeriesKMeans.html\">TSLearn kMeans</A> k-means clustering for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac358e",
   "metadata": {},
   "source": [
    "### Choose optimal number of clusters using scree plot, now for tslearns's kmeans algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5196fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tslearn time series k-means algo - # max - 4 or 8, mean - 3 or 6, nonzero - 2 or 7\n",
    "# Choose best by max value\n",
    "#num_words = 80\n",
    "#columns = df.max().sort_values(ascending=False)[0:num_words].index\n",
    "# Choose best by mean value\n",
    "#num_words = 100\n",
    "#columns = df.mean().sort_values(ascending=False)[0:num_words].index\n",
    "# Choose by number of non-zero values for each word\n",
    "num_words = 150\n",
    "columns = df.astype(bool).sum(axis=0).sort_values(ascending=False)[0:num_words].index\n",
    "\n",
    "subset = df[columns]\n",
    "# Find number of clusters for k-means clustering\n",
    "\n",
    "#create list to hold SSE values for each k\n",
    "sse = []\n",
    "for k in range(1, 15):\n",
    "    tskm = TimeSeriesKMeans(n_clusters=k, random_state=33, metric=\"dtw\")\n",
    "    tskm.fit_predict(subset.T)\n",
    "    sse.append(tskm.inertia_)\n",
    "\n",
    "#visualize results\n",
    "plt.plot(range(1, 15), sse)\n",
    "plt.xticks(range(1, 15))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d12a7",
   "metadata": {},
   "source": [
    "### function to return the tf-idf values for chosen words and the cluster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7936b",
   "metadata": {},
   "source": [
    "### algo: 'skl' for sk-learn and 'ts' for tslearn\n",
    "### method: 'max', 'mean', 'nonzero' as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df, algo, method, cluster_count):\n",
    "    if method == 'max':\n",
    "        # Choose best by max value\n",
    "        num_words = 80\n",
    "        columns = df.max().sort_values(ascending=False)[0:num_words].index\n",
    "    elif method == 'mean':\n",
    "        # Choose best by mean value\n",
    "        num_words = 100\n",
    "        columns = df.mean().sort_values(ascending=False)[0:num_words].index\n",
    "    elif method == 'nonzero':\n",
    "        # Choose by number of non-zero values for each word\n",
    "        num_words = 150\n",
    "        columns = df.astype(bool).sum(axis=0).sort_values(ascending=False)[0:num_words].index\n",
    "    elif method == 'median':\n",
    "        # Choose by median value\n",
    "        num_words = 60\n",
    "        columns = df.median().sort_values(ascending=False)[0:num_words].index\n",
    "    else:\n",
    "        print('Choose a valid method: max, mean, median, nonzero')\n",
    "        return False\n",
    "    subset = df[columns]\n",
    "    labelsdf = pd.DataFrame(columns=['label', 'word'])\n",
    "    if algo == 'skl':\n",
    "        kmeans = KMeans(n_clusters=cluster_count, **kmeans_kwargs).fit(subset.T)\n",
    "        labelsdf['label'] = pd.DataFrame(list(kmeans.labels_))\n",
    "    elif algo == 'ts':\n",
    "        tskm = TimeSeriesKMeans(n_clusters=cluster_count, metric=\"dtw\")\n",
    "        labelsdf['label'] = pd.DataFrame(tskm.fit_predict(subset.T))\n",
    "    else:\n",
    "        print('Choose a valid algo: skl, ts')\n",
    "        return False\n",
    "    labelsdf['word']= list(subset.columns)\n",
    "    subsetdate = subset.copy()\n",
    "    subsetdate['date']= text_df.index\n",
    "    reshape = subsetdate.melt(var_name='word', value_name='tfidf', id_vars=['date'])\n",
    "    reshape['dateint']=reshape['date'].apply(lambda x: x.value)\n",
    "    reshape = pd.merge(reshape, labelsdf, on='word', how='left')\n",
    "    # remove 2004 speech to make plot look nicer\n",
    "    cutoff = pd.to_datetime('2008-01-01')\n",
    "    reshape = reshape.query('date>@cutoff')\n",
    "    reshape.sort_values('date', inplace=True, ignore_index=True)\n",
    "    return([reshape, labelsdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d14c892",
   "metadata": {},
   "source": [
    "### get results for all 6 combos of method and algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sklearn k-means algo - # max - 4 or 5, mean - 9, nonzero - 2 to 5\n",
    "# for tslearn time series k-means algo - # max - 7, mean - 4, nonzero - 2 or 6\n",
    "# def cluster(df, algo, method, cluster_count)\n",
    "reshapemax, labelsdfmax = cluster(df, 'skl', 'max', 9)\n",
    "reshapemean, labelsdfmean = cluster(df, 'skl', 'mean', 9)\n",
    "reshapenonzero, labelsdfnonzero = cluster(df, 'skl', 'nonzero', 9)\n",
    "reshapemedian, labelsdfmedian = cluster(df, 'skl', 'median', 9)\n",
    "reshapetsmax, labelsdftsmax = cluster(df, 'ts', 'max', 7)\n",
    "reshapetsmean, labelsdtsfmean = cluster(df, 'ts', 'mean', 7)\n",
    "reshapetsnonzero, labelsdftsnonzero = cluster(df, 'ts', 'nonzero', 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf39d8",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotthis = reshapemax.query('tfidf > 0.1')\n",
    "fig = px.line(reshapemax, x='date', y='tfidf', color='label', hover_name='word')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdd9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(reshapemean, x=\"date\", y=\"tfidf\", color=\"label\", hover_name=\"word\", \n",
    "                 facet_col=\"label\", trendline='ols')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbea5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(reshapemedian, x=\"date\", y=\"tfidf\", color=\"label\", hover_name=\"word\", \n",
    "                 facet_col=\"label\", trendline='ols')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dcbefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(reshapemedian, x='date', y='tfidf', color='label', hover_name='word')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f6aab",
   "metadata": {},
   "source": [
    "### Cluster lists for the 6 methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8320807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sklearn\n",
    "number = len(set(list(labelsdfmax['label'])))\n",
    "for j in range(number):\n",
    "    print(j)\n",
    "    for i in range(len(labelsdfmax)):\n",
    "        if labelsdfmax['label'].iloc[i] == j:\n",
    "            print('\\t',labelsdfmax['word'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c728a72",
   "metadata": {},
   "source": [
    "### Favorite - subjective judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08955344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean sklearn - includes hope\n",
    "number = len(set(list(labelsdfmean['label'])))\n",
    "for j in range(number):\n",
    "    print(j)\n",
    "    for i in range(len(labelsdfmean)):\n",
    "        if labelsdfmean['label'].iloc[i] == j:\n",
    "            print('\\t',labelsdfmean['word'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median sklearn\n",
    "number = len(set(list(labelsdfmedian['label'])))\n",
    "for j in range(number):\n",
    "    print(j)\n",
    "    for i in range(len(labelsdfmedian)):\n",
    "        if labelsdfmedian['label'].iloc[i] == j:\n",
    "            print('\\t',labelsdfmedian['word'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea61c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-zero sklearn - includes hope\n",
    "number = len(set(list(labelsdfnonzero['label'])))\n",
    "for j in range(number):\n",
    "    print(j)\n",
    "    for i in range(len(labelsdfnonzero)):\n",
    "        if labelsdfnonzero['label'].iloc[i] == j:\n",
    "            print('\\t',labelsdfnonzero['word'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max tslearn\n",
    "number = len(set(list(labelsdftsmax['label'])))\n",
    "for j in range(number):\n",
    "    print(j)\n",
    "    for i in range(len(labelsdftsmax)):\n",
    "        if labelsdftsmax['label'].iloc[i] == j:\n",
    "            print('\\t',labelsdftsmax['word'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9aeb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean tslearn\n",
    "number = len(set(list(labelsdtsfmean['label'])))\n",
    "for j in range(number):\n",
    "    print(j)\n",
    "    for i in range(len(labelsdtsfmean)):\n",
    "        if labelsdtsfmean['label'].iloc[i] == j:\n",
    "            print('\\t',labelsdtsfmean['word'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-zero tslearn\n",
    "number = len(set(list(labelsdftsnonzero['label'])))\n",
    "for j in range(number):\n",
    "    print(j)\n",
    "    for i in range(len(labelsdftsnonzero)):\n",
    "        if labelsdftsnonzero['label'].iloc[i] == j:\n",
    "            print('\\t',labelsdftsnonzero['word'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8548e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222411ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
