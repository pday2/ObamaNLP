{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a9e389",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://www.poynter.org/reporting-editing/2017/why-it-worked-a-rhetorical-analysis-of-obamas-speech-on-race-2/\">Poynter article on the linguistic devices used in Obama's March 18, 2008 speech on American race relations, often referred to as A More Perfect Union</A>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd32e1",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://www.americanrhetoric.com/speeches/barackobamaperfectunion.htm\">A More Perfect Union speech</A>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c156c3",
   "metadata": {},
   "source": [
    "<B>Parallelism</B>\n",
    "<P>\n",
    "At the risk of calling to mind the worst memories of grammar class, I invoke the wisdom that parallel constructions help authors and orators make meaning memorable. To remember how parallelism works, think of equal terms to express equal ideas. So Dr. King dreamed that one day his four children \"will not be judged by the color of their skin but by the content of their character.\" (By the content of their character is parallel to by the color of their skin.)</P>\n",
    "<P>\n",
    "Back to Obama: \"This was one of the tasks we set forth at the beginning of this campaign — to continue the long march of those who came before us, a march for a more just, more equal, more free, more caring and more prosperous America.\" If you are counting, that's five parallel phrases among 43 words. </P>\n",
    "<P>\n",
    "\n",
    "And there are many more:</P>\n",
    "<P>\n",
    "\n",
    " \n",
    "\"…we may not have come from the same place, but we all want to move in the same direction.\"</P>\n",
    "<P>\n",
    "\n",
    " \n",
    "\"So when they are told to bus their children to a school across town; when they hear that an African America is getting an advantage in landing a good job or a spot in a good college because of an injustice that they themselves never committed; when they're told that their fears about crime in urban neighborhoods are somehow prejudiced, resentment builds over time.\"</P>\n",
    "<P>\n",
    "\n",
    " \n",
    "\"…embracing the burdens of our past without becoming victims of our past.\"</P>\n",
    "<P>\n",
    "<I>Roy Peter Clark, October 20, 2017, Poynter<I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06733f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import stanza\n",
    "from collections import defaultdict\n",
    "from textblob import TextBlob\n",
    "from graphviz import Source\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "word_token = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778db7f",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://stanfordnlp.github.io/stanza/getting_started.html\">Stanza quickstart guide</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c2abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Stanford's CoreNLP parser with NLTK\n",
    "# 1. Download CoreNLP from https://stanfordnlp.github.io/CoreNLP/download.html\n",
    "# 2. make sure Java is installed, otherwise download and install Java - https://www.java.com/en/download/windows_manual.jsp\n",
    "# 3. Unzip/extract CoreNLP zip file to a directory\n",
    "# 4. Go to that directory and open a command terminal, and run the following command...\n",
    "# 4b. on my laptop its in C:\\Users\\peter\\stanford-corenlp-4.5.2\n",
    "# 5. java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "# 6. Now for graphviz if you want to view the parse trees, download from https://graphviz.org/download/ then install\n",
    "# 7. Now, can run the following python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f88cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:28:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969bedb0d27544d5970496d51eca383b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:28:27 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-04-06 16:28:27 INFO: Use device: gpu\n",
      "2023-04-06 16:28:27 INFO: Loading: tokenize\n",
      "2023-04-06 16:28:29 INFO: Loading: pos\n",
      "2023-04-06 16:28:30 INFO: Loading: lemma\n",
      "2023-04-06 16:28:30 INFO: Loading: depparse\n",
      "2023-04-06 16:28:30 INFO: Loading: sentiment\n",
      "2023-04-06 16:28:30 INFO: Loading: constituency\n",
      "2023-04-06 16:28:31 INFO: Loading: ner\n",
      "2023-04-06 16:28:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang=\"en\") # Initialize the default English pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558ddad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase1 = \"will not be judged by the color of their skin but by the content of their character.\"\n",
    "phrase2 = \"we may not have come from the same place, but we all want to move in the same direction.\"\n",
    "phrase3 = \"embracing the burdens of our past without becoming victims of our past.\"\n",
    "phrase4 = \"That's one small step for man, one giant leap for mankind.\"\n",
    "phrase5 = \"We may not have come from the same place but we want to move in the same direction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc85363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: embracing\thead id: 0\thead: root\tdeprel: root\n",
      "id: 2\tword: the\thead id: 3\thead: burdens\tdeprel: det\n",
      "id: 3\tword: burdens\thead id: 1\thead: embracing\tdeprel: obj\n",
      "id: 4\tword: of\thead id: 6\thead: past\tdeprel: case\n",
      "id: 5\tword: our\thead id: 6\thead: past\tdeprel: nmod:poss\n",
      "id: 6\tword: past\thead id: 3\thead: burdens\tdeprel: nmod\n",
      "id: 7\tword: without\thead id: 8\thead: becoming\tdeprel: mark\n",
      "id: 8\tword: becoming\thead id: 1\thead: embracing\tdeprel: advcl\n",
      "id: 9\tword: victims\thead id: 8\thead: becoming\tdeprel: obj\n",
      "id: 10\tword: of\thead id: 12\thead: past\tdeprel: case\n",
      "id: 11\tword: our\thead id: 12\thead: past\tdeprel: nmod:poss\n",
      "id: 12\tword: past\thead id: 9\thead: victims\tdeprel: nmod\n",
      "id: 13\tword: .\thead id: 1\thead: embracing\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(phrase3)\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e4a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9bd5f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dep_tree_p4_nopunc.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdp = CoreNLPDependencyParser()\n",
    "sentence = phrase4\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree_p4_nopunc\", format='png')\n",
    "source.view()\n",
    "# Opens in pop-under window... well isn't that nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe91d741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230121.1956)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"568pt\" height=\"479pt\"\n",
       " viewBox=\"0.00 0.00 568.00 479.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 475)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-475 564,-475 564,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-449.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-362.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">5 (step)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219,-435.21C219,-423.8 219,-408.43 219,-395.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"222.5,-395.27 219,-385.27 215.5,-395.27 222.5,-395.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"239\" y=\"-405.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ROOT</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"31\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1 (That)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.81,-358.42C166.47,-352.59 133.26,-342.97 106,-330 90.65,-322.69 74.82,-312.59 61.69,-303.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.03,-300.75 53.87,-297.75 59.95,-306.43 64.03,-300.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"121.5\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.58,-350.52C178.92,-344.73 167.04,-337.63 157,-330 147.32,-322.65 137.65,-313.51 129.43,-305.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.2,-302.89 122.79,-298.03 127.11,-307.7 132.2,-302.89\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">cop</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">3 (one)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M199.73,-348.03C194.83,-342.74 190.09,-336.56 187,-330 183.85,-323.31 182.02,-315.61 180.98,-308.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.47,-308.05 180.04,-298.43 177.5,-308.72 184.47,-308.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nummod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"259\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">4 (small)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M228,-348.01C230.94,-342.32 234.18,-335.92 237,-330 240.51,-322.64 244.14,-314.58 247.42,-307.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"250.54,-308.74 251.31,-298.17 244.12,-305.95 250.54,-308.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"260\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">7 (man)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.21,-349.66C258.51,-343.89 270.01,-337.01 280,-330 291.02,-322.26 302.46,-312.91 312.31,-304.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.51,-307.13 319.7,-297.89 309.88,-301.88 314.51,-307.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"315.5\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"414\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">14 (.)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;14 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.19,-358.68C272.37,-352.87 307.29,-343.18 336,-330 351.88,-322.71 368.33,-312.6 382,-303.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"383.94,-306.3 390.18,-297.74 379.96,-300.54 383.94,-306.3\"/>\n",
       "<text text-anchor=\"middle\" x=\"379\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">punct</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"267\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">6 (for)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M324.78,-261.21C314.42,-248.98 300.22,-232.21 288.48,-218.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.31,-216.28 282.17,-210.91 285.97,-220.81 291.31,-216.28\"/>\n",
       "<text text-anchor=\"middle\" x=\"320.5\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">8 (,)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M339,-261.21C339,-249.8 339,-234.43 339,-221.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"342.5,-221.27 339,-211.27 335.5,-221.27 342.5,-221.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"355\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">punct</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"417\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">11 (leap)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;11 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>7&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M356.93,-261.26C362.81,-255.58 369.3,-249.13 375,-243 382.29,-235.16 389.92,-226.3 396.64,-218.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"399.16,-220.68 402.82,-210.74 393.76,-216.23 399.16,-220.68\"/>\n",
       "<text text-anchor=\"middle\" x=\"406.5\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">appos</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">9 (one)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.25,-174.17C377.37,-169.03 369.37,-162.9 363,-156 356.77,-149.25 351.49,-140.79 347.31,-132.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.58,-131.49 343.1,-123.99 344.27,-134.52 350.58,-131.49\"/>\n",
       "<text text-anchor=\"middle\" x=\"388\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nummod</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"417\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">10 (giant)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M417,-174.21C417,-162.8 417,-147.43 417,-134.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"420.5,-134.27 417,-124.27 413.5,-134.27 420.5,-134.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"433\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">amod</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"515\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">13 (mankind)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>11&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M436.36,-174.21C450.86,-161.63 470.9,-144.25 487.12,-130.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"489.27,-132.95 494.54,-123.75 484.69,-127.66 489.27,-132.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.5\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"515\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">12 (for)</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;12 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M515,-87.21C515,-75.8 515,-60.43 515,-47.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"518.5,-47.27 515,-37.27 511.5,-47.27 518.5,-47.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.5\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x23ca2a59af0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graph image doesn't get saved, need to re-run the code\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f000236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(step That 's one small (man for , (leap one giant (mankind for))) .)\n"
     ]
    }
   ],
   "source": [
    "parse, = sdp.raw_parse(phrase4)\n",
    "print(parse.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "774eb073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('step', 'NN') nsubj ('That', 'DT')\n",
      "('step', 'NN') cop (\"'s\", 'VBZ')\n",
      "('step', 'NN') nummod ('one', 'CD')\n",
      "('step', 'NN') amod ('small', 'JJ')\n",
      "('step', 'NN') nmod ('man', 'NN')\n",
      "('man', 'NN') case ('for', 'IN')\n",
      "('man', 'NN') punct (',', ',')\n",
      "('man', 'NN') appos ('leap', 'NN')\n",
      "('leap', 'NN') nummod ('one', 'CD')\n",
      "('leap', 'NN') amod ('giant', 'JJ')\n",
      "('leap', 'NN') nmod ('mankind', 'NN')\n",
      "('mankind', 'NN') case ('for', 'IN')\n",
      "('step', 'NN') punct ('.', '.')\n"
     ]
    }
   ],
   "source": [
    "for gov, dep, dependent in parse.triples():\n",
    "    print(gov, dep, dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3491b2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            ROOT                                       \n",
      "                             |                                          \n",
      "                             S                                         \n",
      "  ___________________________|_______________________________________   \n",
      " |                           VP                                      | \n",
      " |     ______________________|____                                   |  \n",
      " |    |                           NP                                 | \n",
      " |    |         __________________|___                               |  \n",
      " |    |        |                      PP                             | \n",
      " |    |        |          ____________|____                          |  \n",
      " |    |        |         |                 NP                        | \n",
      " |    |        |         |    _____________|____                     |  \n",
      " |    |        |         |   |    |             NP                   | \n",
      " |    |        |         |   |    |         ____|________            |  \n",
      " |    |        |         |   |    |        |             PP          | \n",
      " |    |        |         |   |    |        |          ___|_____      |  \n",
      " NP   |        NP        |   NP   |        NP        |         NP    | \n",
      " |    |    ____|____     |   |    |    ____|____     |         |     |  \n",
      " DT  VBZ  CD   JJ   NN   IN  NN   ,   CD   JJ   NN   IN        NN    . \n",
      " |    |   |    |    |    |   |    |   |    |    |    |         |     |  \n",
      "That  's one small step for man   ,  one giant leap for     mankind  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = CoreNLPParser()\n",
    "sent, = parser.parse_text(phrase4)\n",
    "sent.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent.pos()\n",
    "#sent.productions()\n",
    "#sent.pformat_latex_qtree() #compatible with LaTeX qtree package\n",
    "#sent.height()\n",
    "for level in range(sent.height()):\n",
    "    print(sent[level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3413c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "listr = []\n",
    "def iterate(tree):\n",
    "    if len(tree) > 1:\n",
    "        for i in range(len(tree)):\n",
    "            iterate(tree[i])\n",
    "    else:\n",
    "        listr.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate(sent[0])\n",
    "listr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/barackobamaperfectunion.txt') as f:\n",
    "    text = f.read()\n",
    "sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parallels(text):\n",
    "    count = 0\n",
    "    sents = sent_tokenize(text)\n",
    "    for phrase in sents:\n",
    "        try:\n",
    "            sent, = parser.parse_text(phrase)\n",
    "        except:\n",
    "            #print('----- PARSE ERROR -----')\n",
    "            #print(phrase)\n",
    "            continue\n",
    "        poss = []\n",
    "        words = []\n",
    "        for word in sent.pos():\n",
    "            poss.append(word[1])\n",
    "            words.append(word[0])\n",
    "        #print(words)\n",
    "        stop = False\n",
    "        results = []\n",
    "        for length in range(7,3,-1):\n",
    "            length = min(length, len(words))\n",
    "            for i in range(len(poss)-length+1):\n",
    "                for j in range(len(poss)-length+1):\n",
    "                    if abs(i-j) > length:\n",
    "                        if poss[i:i+length]==poss[j:j+length]:\n",
    "                            if length > 4 or (',' not in poss[i:i+length] and '``' not in poss[i:i+length]):\n",
    "                                results.append([i,j,length])\n",
    "                                count += 1\n",
    "                                stop = True\n",
    "                                break\n",
    "                if stop: break\n",
    "            if stop: break\n",
    "        '''\n",
    "        for result in results:\n",
    "            print(words[result[0]:result[0]+result[2]])\n",
    "            print(words[result[1]:result[1]+result[2]])\n",
    "            print()\n",
    "        '''\n",
    "    return(len(sents), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a83227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22040\\2984996246.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oba_text['sent_count'] = 0\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22040\\2984996246.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oba_text['parallel_count'] = 0\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22040\\2984996246.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gwb_text['sent_count'] = 0\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_22040\\2984996246.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gwb_text['parallel_count'] = 0\n"
     ]
    }
   ],
   "source": [
    "oba = pd.read_csv('text_sentences_words.csv')\n",
    "gwb = pd.read_csv('text_sentences_words_gwb.csv')\n",
    "oba = oba.query('source == \"oba\"')\n",
    "oba_text = oba[['text', 'date']]\n",
    "oba_text['sent_count'] = 0\n",
    "oba_text['parallel_count'] = 0\n",
    "gwb_text = gwb[['text', 'date']]\n",
    "gwb_text['sent_count'] = 0\n",
    "gwb_text['parallel_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gwb_text.text)):\n",
    "    sent_count, parallel_count = count_parallels(gwb_text.text[i])\n",
    "    gwb_text['sent_count'].iloc[i] = sent_count\n",
    "    gwb_text['parallel_count'].iloc[i] = parallel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0980e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwb_text['parallel_per_sent']=gwb_text.parallel_count/gwb_text.sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac82046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gwb_text.to_csv('parallelism_gwb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(oba_text.text)):\n",
    "    sent_count, parallel_count = count_parallels(oba_text.text[i])\n",
    "    oba_text['sent_count'].iloc[i] = sent_count\n",
    "    oba_text['parallel_count'].iloc[i] = parallel_count\n",
    "oba_text['parallel_per_sent']=oba_text.parallel_count/oba_text.sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "oba_text.to_csv('parallelism_oba.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aaa488",
   "metadata": {},
   "source": [
    "### Find Parallelism in a specific speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e58863f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './Data/barackobamaselma50anniversarymarch.txt'\n",
    "with open(file, encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5df9006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', ',', 'God', 'will', 'take', 'care', 'of']\n",
      "['breast', ',', 'God', 'will', 'take', 'care', 'of']\n",
      "\n",
      "['for', 'a', 'night', 'behind']\n",
      "['of', 'the', 'church', 'on']\n",
      "\n",
      "[',', 'Independence', 'Hall', 'and', 'Seneca']\n",
      "[',', 'Kitty', 'Hawk', 'and', 'Cape']\n",
      "\n",
      "['turbulent', 'history', ',', 'the', 'stain', 'of', 'slavery']\n",
      "['civil', 'war', ';', 'the', 'yoke', 'of', 'segregation']\n",
      "\n",
      "['a', 'clash', 'of', 'armies']\n",
      "['a', 'clash', 'of', 'wills']\n",
      "\n",
      "['John', 'Lewis', ',', 'Joseph', 'Lowery', ',', 'Hosea']\n",
      "['Amelia', 'Boynton', ',', 'Diane', 'Nash', ',', 'Ralph']\n",
      "\n",
      "['and', 'a', 'fair', 'America', ',', 'an']\n",
      "['and', 'a', 'generous', 'America', ',', 'that']\n",
      "\n",
      "----- PARSE ERROR -----\n",
      "They did as Scripture instructed: “Rejoice in hope, be patient in tribulation, be constant in prayer.” And in the days to come, they went back again and again.\n",
      "----- PARSE ERROR -----\n",
      "And he would send them protection, and speak to the nation, echoing their call for America and the world to hear: “We shall overcome.” What enormous faith these men and women had.\n",
      "['was', 'questioned', ';', 'their']\n",
      "['were', 'threatened', ';', 'their']\n",
      "\n",
      "['greater', 'expression', 'of', 'faith']\n",
      "['greater', 'form', 'of', 'patriotism']\n",
      "\n",
      "----- PARSE ERROR -----\n",
      "It is instead the manifestation of a creed written into our founding documents: “We the People…in order to form a more perfect union.” “We hold these truths to be self-evident, that all men are created equal.” These are not just words.\n",
      "['a', 'call', 'to', 'action']\n",
      "['a', 'roadmap', 'for', 'citizenship']\n",
      "\n",
      "['For', 'founders', 'like', 'Franklin', 'and', 'Jefferson']\n",
      "['for', 'leaders', 'like', 'Lincoln', 'and', 'FDR']\n",
      "\n",
      "['The', 'American', 'instinct', 'that', 'led']\n",
      "['the', 'same', 'instinct', 'that', 'moved']\n",
      "\n",
      "['the', 'same', 'instinct', 'that', 'drew', 'immigrants']\n",
      "['the', 'same', 'instinct', 'that', 'led', 'women']\n",
      "\n",
      "['from', 'the', 'Congressional', 'Black']\n",
      "['to', 'the', 'Oval', 'Office']\n",
      "\n",
      "['by', 'reasserting', 'the', 'past']\n",
      "['by', 'transcending', 'the', 'past']\n",
      "\n",
      "----- PARSE ERROR -----\n",
      "“We are capable of bearing a great burden,” James Baldwin once wrote, “once we discover that the burden is reality and arrive where reality is.” There’s nothing America can’t handle if we actually look squarely at the problem.\n",
      "['laws', 'can', 'be', 'passed', ',']\n",
      "['consciences', 'can', 'be', 'stirred', ',']\n",
      "\n",
      "['and', 'overcrowded', 'prisons', ',', 'and']\n",
      "['and', 'good', 'workers', ',', 'and']\n",
      "\n",
      "['And', 'if', 'we', 'really', 'mean', 'it']\n",
      "['but', 'if', 'we', 'really', 'mean', 'it']\n",
      "\n",
      "['the', 'dignity', 'of', 'a']\n",
      "['that', 'ladder', 'into', 'the']\n",
      "\n",
      "[',', 'the', 'culmination', 'of', 'so', 'much', 'blood']\n",
      "[',', 'the', 'product', 'of', 'so', 'much', 'sacrifice']\n",
      "\n",
      "['to', 'honor', 'this', 'day']\n",
      "['to', 'restore', 'that', 'law']\n",
      "\n",
      "['the', 'number', 'of', 'jellybeans', 'in', 'a', 'jar']\n",
      "['the', 'number', 'of', 'bubbles', 'on', 'a', 'bar']\n",
      "\n",
      "['or', 'a', 'Unitarian', 'minister']\n",
      "['or', 'a', 'young', 'mother']\n",
      "\n",
      "['the', 'firefighters', 'who', 'rushed']\n",
      "['the', 'volunteers', 'who', 'signed']\n",
      "\n",
      "['bluegrass', 'and', 'country', ',', 'and']\n",
      "['rock', 'and', 'roll', ',', 'and']\n",
      "\n",
      "----- PARSE ERROR -----\n",
      "We are the people Langston Hughes wrote of who “build our temples for tomorrow, strong as we know how.” We are the people Emerson wrote of, “who for truth and honor’s sake stand fast and suffer long;” who are “never tired, so long as we can see far enough.” That’s what America is.\n",
      "['all', 'across', 'the', 'country']\n",
      "['away', 'from', 'this', 'day']\n",
      "\n",
      "['steps', 'to', 'be', 'taken']\n",
      "['bridges', 'to', 'be', 'crossed']\n",
      "\n",
      "----- PARSE ERROR -----\n",
      "Because the single-most powerful word in our democracy is the word “We.” “We The People.” “We Shall Overcome.” “Yes We Can.” That word is owned by no one.\n",
      "['we', 'believe', 'in', 'the', 'power']\n",
      "['we', 'believe', 'in', 'this', 'country']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(text)\n",
    "count = 0\n",
    "for phrase in sents:\n",
    "    try:\n",
    "        sent, = parser.parse_text(phrase)\n",
    "    except:\n",
    "        print('----- PARSE ERROR -----')\n",
    "        print(phrase)\n",
    "        continue\n",
    "    poss = []\n",
    "    words = []\n",
    "    for word in sent.pos():\n",
    "        poss.append(word[1])\n",
    "        words.append(word[0])\n",
    "\n",
    "    stop = False\n",
    "    results = []\n",
    "    for length in range(7,3,-1):\n",
    "        length = min(length, len(words))\n",
    "        for i in range(len(poss)-length+1):\n",
    "            for j in range(len(poss)-length+1):\n",
    "                if abs(i-j) > length:\n",
    "                    if poss[i:i+length]==poss[j:j+length]:\n",
    "                        if length > 4 or (',' not in poss[i:i+length] and '``' not in poss[i:i+length]):\n",
    "                            results.append([i,j,length])\n",
    "                            count += 1\n",
    "                            stop = True\n",
    "                            break\n",
    "            if stop: break\n",
    "        if stop: break\n",
    "    \n",
    "    for result in results:\n",
    "        print(words[result[0]:result[0]+result[2]])\n",
    "        print(words[result[1]:result[1]+result[2]])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1e7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Meanwhile, the Voting Rights Act, the culmination of so much blood, so much sweat and tears, the product of so much sacrifice in the face of wanton violence, the Voting Rights Act stands weakened, its future subject to political rancor.'\n",
    "sent2 = 'It’s the same instinct that drew immigrants from across oceans and the Rio Grande; the same instinct that led women to reach for the ballot, workers to organize against an unjust status quo; the same instinct that led us to plant a flag at Iwo Jima and on the surface of the Moon.'\n",
    "sent3 = 'In one afternoon 50 years ago, so much of our turbulent history, the stain of slavery and anguish of civil war; the yoke of segregation and tyranny of Jim Crow; the death of four little girls in Birmingham; and the dream of a Baptist preacher, all that history met on this bridge.'\n",
    "sent4 = 'For everywhere in this country, there are first steps to be taken, there’s new ground to cover, there are more bridges to be crossed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4b73d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dep_tree_culmination_product.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdp = CoreNLPDependencyParser()\n",
    "sentence = sent1\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree_culmination_product\", format='png')\n",
    "source.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392b9116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dep_tree_instinct.png'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sent2\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree_instinct\", format='png')\n",
    "source.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "def439e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dep_tree_turbulent_history.png'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sent3\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree_turbulent_history\", format='png')\n",
    "source.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ff0eaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dep_tree_steps_to_be_taken.png'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sent4\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree_steps_to_be_taken\", format='png')\n",
    "source.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "189e2b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                 ROOT                                                                                        \n",
      "                                                                                  |                                                                                           \n",
      "                                                                                  S                                                                                          \n",
      "         _________________________________________________________________________|________________________________________________________________________________________   \n",
      "        |                                                              PRN                                                   |               |                             | \n",
      "        |                             __________________________________|_______________________________________________     |               |                             |  \n",
      "        |                            |         S                                                                        |    |               |                             | \n",
      "        |                            |     ____|____                                                                    |    |               |                             |  \n",
      "        |                            |    |         VP                                                                  |    |               |                             | \n",
      "        |                            |    |     ____|_____                                                              |    |               |                             |  \n",
      "        |                            |    |    |          NP                                                            |    |               |                             | \n",
      "        |                            |    |    |     _____|________                                                     |    |               |                             |  \n",
      "        |                            |    |    |    |     |        S                                                    |    |               |                             | \n",
      "        |                            |    |    |    |     |        |                                                    |    |               |                             |  \n",
      "        |                            |    |    |    |     |        VP                                                   |    |               |                             | \n",
      "        |                            |    |    |    |     |     ___|_________                                           |    |               |                             |  \n",
      "        |                            |    |    |    |     |    |             VP                                         |    |               VP                            | \n",
      "        |                            |    |    |    |     |    |    _________|_____________                             |    |     __________|_________                    |  \n",
      "        |                            |    |    |    |     |    |   |                       VP                           |    |    |   |                NP                  | \n",
      "        |                            |    |    |    |     |    |   |     __________________|___                         |    |    |   |       _________|___                |  \n",
      "        |                            |    |    |    |     |    |   |    |    |                 S                        |    |    |   |      |             S               | \n",
      "        |                            |    |    |    |     |    |   |    |    |              ___|______________          |    |    |   |      |             |               |  \n",
      "        PP                           |    |    |    |     |    |   |    |    |             NP                 |         |    |    |   |      |             VP              | \n",
      "  ______|_______                     |    |    |    |     |    |   |    |    |          ___|________          |         |    |    |   |      |      _______|___            |  \n",
      " |              PP                   |    |    |    |     |    |   |    |    |         NP      |    |         VP        |    |    |   |      |     |           VP          | \n",
      " |       _______|________            |    |    |    |     |    |   |    |    |     ____|___    |    |      ___|____     |    |    |   |      |     |        ___|_____      |  \n",
      " |     ADVP     |        NP          |    NP   |    |     |    |   |    |    |    NP       |   |    |     |        VP   |    NP   |  ADVP    |     |       |         VP    | \n",
      " |      |       |    ____|_____      |    |    |    |     |    |   |    |    |    |        |   |    |     |        |    |    |    |   |      |     |       |         |     |  \n",
      " IN     RB      IN  DT         NN    ,    EX  VBP   JJ   NNS   TO  VB  VBN   ,    EX      POS  JJ   NN    TO       VB   ,    EX  VBP RBR    NNS    TO      VB       VBN    . \n",
      " |      |       |   |          |     |    |    |    |     |    |   |    |    |    |        |   |    |     |        |    |    |    |   |      |     |       |         |     |  \n",
      "For everywhere  in this     country  ,  there are first steps  to  be taken  ,  there      ’s new ground  to     cover  ,  there are more bridges  to      be     crossed  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = CoreNLPParser()\n",
    "sent, = parser.parse_text(sent4)\n",
    "sent.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b16ea4",
   "metadata": {},
   "source": [
    "### Add parallelism data to tidy data, maybe create an obama only tidy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c625df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_oba = pd.read_csv('parallelism_oba.csv')\n",
    "par_gwb = pd.read_csv('parallelism_gwb.csv')\n",
    "par_oba.drop(['text', 'sent_count'], axis=1, inplace=True)\n",
    "par_gwb.drop(['text', 'sent_count'], axis=1, inplace=True)\n",
    "tidy = pd.read_csv('tidy_data.csv')\n",
    "tidy_gwb = pd.read_csv('tidy_data_gwb.csv')\n",
    "tidy_oba = tidy.query('source == \"oba\"')\n",
    "tidy_oba = pd.merge(tidy_oba, par_oba, how='left', on='date')\n",
    "tidy_gwb = pd.merge(tidy_gwb, par_gwb, how='left', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f524357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tidy_gwb.columns) == set(tidy_oba.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6a5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy_oba.to_csv('tidy_data_oba.csv', index=False)\n",
    "#tidy_gwb.to_csv('tidy_data_gwb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf78b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
