{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a9e389",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://www.poynter.org/reporting-editing/2017/why-it-worked-a-rhetorical-analysis-of-obamas-speech-on-race-2/\">Poynter article on the linguistic devices used in Obama's March 18, 2008 speech on American race relations, often referred to as A More Perfect Union</A>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd32e1",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://www.americanrhetoric.com/speeches/barackobamaperfectunion.htm\">A More Perfect Union speech</A>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c156c3",
   "metadata": {},
   "source": [
    "<B>Parallelism</B>\n",
    "<P>\n",
    "At the risk of calling to mind the worst memories of grammar class, I invoke the wisdom that parallel constructions help authors and orators make meaning memorable. To remember how parallelism works, think of equal terms to express equal ideas. So Dr. King dreamed that one day his four children \"will not be judged by the color of their skin but by the content of their character.\" (By the content of their character is parallel to by the color of their skin.)</P>\n",
    "<P>\n",
    "Back to Obama: \"This was one of the tasks we set forth at the beginning of this campaign — to continue the long march of those who came before us, a march for a more just, more equal, more free, more caring and more prosperous America.\" If you are counting, that's five parallel phrases among 43 words. </P>\n",
    "<P>\n",
    "\n",
    "And there are many more:</P>\n",
    "<P>\n",
    "\n",
    " \n",
    "\"…we may not have come from the same place, but we all want to move in the same direction.\"</P>\n",
    "<P>\n",
    "\n",
    " \n",
    "\"So when they are told to bus their children to a school across town; when they hear that an African America is getting an advantage in landing a good job or a spot in a good college because of an injustice that they themselves never committed; when they're told that their fears about crime in urban neighborhoods are somehow prejudiced, resentment builds over time.\"</P>\n",
    "<P>\n",
    "\n",
    " \n",
    "\"…embracing the burdens of our past without becoming victims of our past.\"</P>\n",
    "<P>\n",
    "<I>Roy Peter Clark, October 20, 2017, Poynter<I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06733f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/muddy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import stanza\n",
    "from collections import defaultdict\n",
    "from textblob import TextBlob\n",
    "from graphviz import Source\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "word_token = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778db7f",
   "metadata": {},
   "source": [
    "### <A HREF=\"https://stanfordnlp.github.io/stanza/getting_started.html\">Stanza quickstart guide</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Stanford's CoreNLP parser with NLTK\n",
    "# 1. Download CoreNLP from https://stanfordnlp.github.io/CoreNLP/download.html\n",
    "# 2. make sure Java is installed, otherwise download and install Java - https://www.java.com/en/download/windows_manual.jsp\n",
    "# 3. Unzip/extract CoreNLP zip file to a directory\n",
    "# 4. Go to that directory and open a command terminal, and run the following command...\n",
    "# 4b. on my laptop its in C:\\Users\\peter\\stanford-corenlp-4.5.2\n",
    "# 5. java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "# 6. Now for graphviz if you want to view the parse trees, download from https://graphviz.org/download/ then install\n",
    "# 7. Now, can run the following python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f88cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang=\"en\") # Initialize the default English pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ddad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase1 = \"will not be judged by the color of their skin but by the content of their character.\"\n",
    "phrase2 = \"we may not have come from the same place, but we all want to move in the same direction.\"\n",
    "phrase3 = \"embracing the burdens of our past without becoming victims of our past.\"\n",
    "phrase4 = \"That's one small step for man, one giant leap for mankind.\"\n",
    "phrase5 = \"We may not have come from the same place but we want to move in the same direction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc85363",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(phrase3)\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdp = CoreNLPDependencyParser()\n",
    "sentence = phrase4\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree_p4_nopunc\", format='png')\n",
    "source.view()\n",
    "# Opens in pop-under window... well isn't that nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph image doesn't get saved, need to re-run the code\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse, = sdp.raw_parse(phrase4)\n",
    "print(parse.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774eb073",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gov, dep, dependent in parse.triples():\n",
    "    print(gov, dep, dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser()\n",
    "sent, = parser.parse_text(phrase4)\n",
    "sent.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent.pos()\n",
    "#sent.productions()\n",
    "#sent.pformat_latex_qtree() #compatible with LaTeX qtree package\n",
    "#sent.height()\n",
    "for level in range(sent.height()):\n",
    "    print(sent[level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3413c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "listr = []\n",
    "def iterate(tree):\n",
    "    if len(tree) > 1:\n",
    "        for i in range(len(tree)):\n",
    "            iterate(tree[i])\n",
    "    else:\n",
    "        listr.append(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate(sent[0])\n",
    "listr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/barackobamaperfectunion.txt') as f:\n",
    "    text = f.read()\n",
    "sents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parallels(text):\n",
    "    count = 0\n",
    "    sents = sent_tokenize(text)\n",
    "    for phrase in sents:\n",
    "        try:\n",
    "            sent, = parser.parse_text(phrase)\n",
    "        except:\n",
    "            #print('----- PARSE ERROR -----')\n",
    "            #print(phrase)\n",
    "            continue\n",
    "        poss = []\n",
    "        words = []\n",
    "        for word in sent.pos():\n",
    "            poss.append(word[1])\n",
    "            words.append(word[0])\n",
    "        #print(words)\n",
    "        stop = False\n",
    "        results = []\n",
    "        for length in range(7,3,-1):\n",
    "            length = min(length, len(words))\n",
    "            for i in range(len(poss)-length+1):\n",
    "                for j in range(len(poss)-length+1):\n",
    "                    if abs(i-j) > length:\n",
    "                        if poss[i:i+length]==poss[j:j+length]:\n",
    "                            if length > 4 or (',' not in poss[i:i+length] and '``' not in poss[i:i+length]):\n",
    "                                results.append([i,j,length])\n",
    "                                count += 1\n",
    "                                stop = True\n",
    "                                break\n",
    "                if stop: break\n",
    "            if stop: break\n",
    "        '''\n",
    "        for result in results:\n",
    "            print(words[result[0]:result[0]+result[2]])\n",
    "            print(words[result[1]:result[1]+result[2]])\n",
    "            print()\n",
    "        '''\n",
    "    return(len(sents), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oba = pd.read_csv('text_sentences_words.csv')\n",
    "gwb = pd.read_csv('text_sentences_words_gwb.csv')\n",
    "oba = oba.query('source == \"oba\"')\n",
    "oba_text = oba[['text', 'date']]\n",
    "oba_text['sent_count'] = 0\n",
    "oba_text['parallel_count'] = 0\n",
    "gwb_text = gwb[['text', 'date']]\n",
    "gwb_text['sent_count'] = 0\n",
    "gwb_text['parallel_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gwb_text.text)):\n",
    "    sent_count, parallel_count = count_parallels(gwb_text.text[i])\n",
    "    gwb_text['sent_count'].iloc[i] = sent_count\n",
    "    gwb_text['parallel_count'].iloc[i] = parallel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0980e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwb_text['parallel_per_sent']=gwb_text.parallel_count/gwb_text.sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac82046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gwb_text.to_csv('parallelism_gwb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(oba_text.text)):\n",
    "    sent_count, parallel_count = count_parallels(oba_text.text[i])\n",
    "    oba_text['sent_count'].iloc[i] = sent_count\n",
    "    oba_text['parallel_count'].iloc[i] = parallel_count\n",
    "oba_text['parallel_per_sent']=oba_text.parallel_count/oba_text.sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "oba_text.to_csv('parallelism_oba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58863f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=gwb_text.text[4]\n",
    "sents = sent_tokenize(text)\n",
    "count = 0\n",
    "for phrase in sents:\n",
    "    try:\n",
    "        sent, = parser.parse_text(phrase)\n",
    "    except:\n",
    "        print('----- PARSE ERROR -----')\n",
    "        print(phrase)\n",
    "        continue\n",
    "    poss = []\n",
    "    words = []\n",
    "    for word in sent.pos():\n",
    "        poss.append(word[1])\n",
    "        words.append(word[0])\n",
    "\n",
    "    stop = False\n",
    "    results = []\n",
    "    for length in range(7,3,-1):\n",
    "        length = min(length, len(words))\n",
    "        for i in range(len(poss)-length+1):\n",
    "            for j in range(len(poss)-length+1):\n",
    "                if abs(i-j) > length:\n",
    "                    if poss[i:i+length]==poss[j:j+length]:\n",
    "                        if length > 4 or (',' not in poss[i:i+length] and '``' not in poss[i:i+length]):\n",
    "                            results.append([i,j,length])\n",
    "                            count += 1\n",
    "                            stop = True\n",
    "                            break\n",
    "            if stop: break\n",
    "        if stop: break\n",
    "    \n",
    "    for result in results:\n",
    "        print(words[result[0]:result[0]+result[2]])\n",
    "        print(words[result[1]:result[1]+result[2]])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add parallelism data to tidy data, maybe create an obama only tidy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c625df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "par_oba = pd.read_csv('parallelism_oba.csv')\n",
    "par_gwb = pd.read_csv('parallelism_gwb.csv')\n",
    "par_oba.drop(['text', 'sent_count'], axis=1, inplace=True)\n",
    "par_gwb.drop(['text', 'sent_count'], axis=1, inplace=True)\n",
    "tidy = pd.read_csv('tidy_data.csv')\n",
    "tidy_gwb = pd.read_csv('tidy_data_gwb.csv')\n",
    "tidy_oba = tidy.query('source == \"oba\"')\n",
    "tidy_oba = pd.merge(tidy_oba, par_oba, how='left', on='date')\n",
    "tidy_gwb = pd.merge(tidy_gwb, par_gwb, how='left', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f524357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tidy_gwb.columns) == set(tidy_oba.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6a5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy_oba.to_csv('tidy_data_oba.csv', index=False)\n",
    "#tidy_gwb.to_csv('tidy_data_gwb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf78b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
