{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7f9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dependency parse tree depth\n",
    "# Using Stanford's CoreNLP parser with NLTK\n",
    "# 1. Download CoreNLP from https://stanfordnlp.github.io/CoreNLP/download.html\n",
    "# 2. make sure Java is installed, otherwise download and install Java - https://www.java.com/en/download/windows_manual.jsp\n",
    "# 3. Unzip/extract CoreNLP zip file to a directory\n",
    "# 4. Go to that directory and open a command terminal, and run the following command...\n",
    "# 4b. on my laptop its in C:\\Users\\peter\\stanford-corenlp-4.5.2\n",
    "# 5. java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "# 6. Now for graphviz if you want to view the parse trees, download from https://graphviz.org/download/ then install\n",
    "# 7. Now, can run the following python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671cad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d6c2823fc84c04abe8dbc732dcf61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 13:17:15 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-05-01 13:17:16 INFO: File exists: C:\\Users\\peter\\stanza_resources\\en\\default.zip\n",
      "2023-05-01 13:17:20 INFO: Finished downloading models and saved to C:\\Users\\peter\\stanza_resources.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import regex as re\n",
    "import os\n",
    "import sys\n",
    "import unicodedata\n",
    "import re\n",
    "import stanza\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import cmudict\n",
    "stanza.download('en')\n",
    "nltk.download('punkt')\n",
    "word_token = TreebankWordTokenizer()\n",
    "from nrclex import NRCLex\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download ('wordnet')\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c680b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff70b6b67eb2440192a98d6d3d337eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 13:17:20 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-05-01 13:17:21 INFO: File exists: C:\\Users\\peter\\stanza_resources\\en\\default.zip\n",
      "2023-05-01 13:17:25 INFO: Finished downloading models and saved to C:\\Users\\peter\\stanza_resources.\n",
      "2023-05-01 13:17:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d486ed7b62c4fd687bb7a2464c45630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 13:17:25 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-05-01 13:17:25 INFO: Use device: gpu\n",
      "2023-05-01 13:17:25 INFO: Loading: tokenize\n",
      "2023-05-01 13:17:28 INFO: Loading: pos\n",
      "2023-05-01 13:17:28 INFO: Loading: lemma\n",
      "2023-05-01 13:17:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en', processors='tokenize, lemma, pos')\n",
    "nlp_spacy = spacy.load(\"en_core_web_md\")\n",
    "stopwords = pd.read_table('./Data/word_lists/kaggle_stopwords.txt')\n",
    "#paths = ['./Data/Top10/']\n",
    "#suffix = 'topten'\n",
    "#source = 'topten'\n",
    "#dates_file = 'datefiles_topten.csv'\n",
    "paths = ['./Data/amrhet/']\n",
    "suffix = 'amrhet'\n",
    "source = suffix\n",
    "dates_file = 'datetitle.csv'\n",
    "dates = pd.read_csv(dates_file)\n",
    "dates['date'] = pd.to_datetime(dates['date'], format='%Y-%m-%d')\n",
    "# date, title, file\n",
    "try:\n",
    "    dates.rename(columns={\"url\":\"file\"}, inplace=True)\n",
    "except:\n",
    "    print()\n",
    "try:\n",
    "    dates = dates.drop('title', axis=1)\n",
    "except:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a61d4cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------LOADING DOCUMENTS----------\n",
      "Length of text_df: 433\n"
     ]
    }
   ],
   "source": [
    "print('---------LOADING DOCUMENTS----------')\n",
    "# Load up the speeches\n",
    "speeches = []\n",
    "for path in paths:\n",
    "    list_of_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                list_of_files.append(os.path.join(root,file))\n",
    "\n",
    "    for file in list_of_files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        speeches.append(text)\n",
    "\n",
    "#clean out goofy unicode  space characters \n",
    "speeches = [unicodedata.normalize(\"NFKD\", speech) for speech in speeches if len(speech)>0 ]\n",
    "#clean out xa0 space characters\n",
    "[speech.replace(u'\\xa0', '') for speech in speeches]; # ; supresses output\n",
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "speeches = [remove_bracket(speech) for speech in speeches]\n",
    "# Clean up whitespace\n",
    "speeches = [re.sub('[\\s+]', ' ', speech) for speech in speeches]\n",
    "# Remove -- that's all over the amrhet files\n",
    "def remove_dashes(text):\n",
    "    return re.sub(r'-- ', '', text)\n",
    "speeches = [remove_dashes(speech) for speech in speeches]\n",
    "text_df = pd.DataFrame({'file' : list_of_files,\n",
    "                        'text' : speeches})\n",
    "\n",
    "text_df = pd.merge(text_df, dates, how='inner', on='file')\n",
    "text_df = text_df.sort_values(by='date', ignore_index=True)\n",
    "text_df = text_df[['date', 'file', 'text']]\n",
    "text_df['source'] = 'oba'\n",
    "text_df.set_index('file', inplace=True)\n",
    "text_df['sentences'] = text_df['text'].apply(sent_tokenize)\n",
    "text_df['words'] = text_df['text'].apply(word_token.tokenize)\n",
    "text_df['word_set'] = text_df['words'].apply(set)\n",
    "text_df['num_sents'] = text_df['sentences'].apply(len)\n",
    "text_df['num_words'] = text_df['words'].apply(len)\n",
    "text_df['num_unique_words'] = text_df['word_set'].apply(len)\n",
    "print(\"Length of text_df:\", len(text_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db97d6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_df.shape =  (54992, 6)\n"
     ]
    }
   ],
   "source": [
    "# Break up into one row per sentence\n",
    "text_df=text_df.explode('sentences')\n",
    "text_df.drop(['text', 'words', 'word_set'] , axis=1, inplace=True)\n",
    "text_df.rename(columns={\"sentences\": \"text\"}, inplace=True)\n",
    "print(\"text_df.shape = \", text_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778f13f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------POS TAGGING---------\n",
      "...pst, this is slow\n",
      "text_df.shape =  (54992, 23)\n",
      "----------------------DONE!!!!!!!!!!!!-------------------\n"
     ]
    }
   ],
   "source": [
    "############# POS TAGGING ###################  NEW TRY!!!!\n",
    "print('---------POS TAGGING---------')\n",
    "\n",
    "# The nlp(text) uses a lot of gpu memory, causes errors sometimes, may need to restart notebook to freshen things up\n",
    "parts_of_speech = ['NUM','ADV','SYM','NOUN','ADP','PROPN','DET','INTJ','AUX',\n",
    "                   'CCONJ','ADJ','PRON','SCONJ','X','VERB','PUNCT','PART']\n",
    "for col in parts_of_speech:\n",
    "    text_df[col] = 0\n",
    "\n",
    "print('...pst, this is slow')\n",
    "for i, text in enumerate(text_df.text):\n",
    "    doc = nlp_stanza(text) # Run stanza on each speech\n",
    "    mat_of_pos = [[word.pos for word in sentence.words] for sentence in doc.sentences] # matrix of POS for each sentence\n",
    "    # How to flatten a list = [item for sublist in list_of_lists for item in sublist]\n",
    "    list_of_pos = [pos for sentence in mat_of_pos for pos in sentence] # flatten matrix into one list of all pos\n",
    "    total_pos_count = len(list_of_pos)\n",
    "    for pos in parts_of_speech:\n",
    "        #dfd.iloc[[0, 2], dfd.columns.get_loc('A')]\n",
    "        text_df.iloc[i, text_df.columns.get_loc(pos)] = list_of_pos.count(pos)/total_pos_count\n",
    "        \n",
    "print(\"text_df.shape = \", text_df.shape)\n",
    "print(\"----------------------DONE!!!!!!!!!!!!-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b189aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------NRCLex EMOTION TAGGING---------\n",
      "(54992, 25)\n"
     ]
    }
   ],
   "source": [
    "print('---------NRCLex EMOTION TAGGING---------')\n",
    "text_df['emo'] = text_df.text.apply(NRCLex)\n",
    "\n",
    "for i in range(len(text_df)):\n",
    "    anticip = text_df.emo[i].affect_frequencies.pop('anticip')\n",
    "print(text_df.shape)\n",
    "\n",
    "# Get names of emotion attributes, locate and remove anticipation as it seems to alway be 0\n",
    "attributes = ['fear','anger','trust','surprise','positive','negative',\n",
    "            'sadness','disgust','joy','anticipation']\n",
    "text_df = text_df.reindex(columns=text_df.columns.tolist() + attributes)\n",
    "for i in range(len(text_df)):\n",
    "    for attr in attributes:\n",
    "        try:\n",
    "            value = text_df.emo[i].affect_frequencies[attr]\n",
    "        except:\n",
    "            value = 0\n",
    "        text_df.iloc[i, text_df.columns.get_loc(attr)] = value\n",
    "\n",
    "print(\"text_df.shape = \", text_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5420662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------TextBlob----------\n",
      "text_df.shape =  (54992, 37)\n"
     ]
    }
   ],
   "source": [
    "print('----------TextBlob----------')\n",
    "text_df['TBsubjectivity']=[TextBlob(text).sentiment.subjectivity for text in text_df['text']]\n",
    "text_df['TBpolarity']=[TextBlob(text).sentiment.polarity for text in text_df['text']]\n",
    "print(\"text_df.shape = \", text_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0bcf549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------READABILITY----------\n",
      "text_df.shape =  (54992, 56)\n"
     ]
    }
   ],
   "source": [
    "print('----------READABILITY----------')\n",
    "\n",
    "########## HELPER FUNCTIONS #############\n",
    "def words_per_sentence(sentence):\n",
    "    '''returns: integer number of words in a sentence'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return(len(tokens))\n",
    "\n",
    "def chars_per_word(word):\n",
    "    '''returns: integer number of characters in a word'''\n",
    "    return(len(word))\n",
    "\n",
    "def string_to_list(sentence):\n",
    "    '''converts a string/sentence to a list of words'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return(tokenizer.tokenize(sentence))\n",
    "\n",
    "def chars_per_word_sentence(sentence):\n",
    "    '''input: string of a sentence\n",
    "       returns: list of number of characters in a sentence'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    char_len_list = [chars_per_word(word) for word in tokens]\n",
    "    return(char_len_list)\n",
    "\n",
    "def text_to_sentence(text):\n",
    "    '''uses spacy nlp object to break up sentences\n",
    "       input: pandas series of strings\n",
    "       returns: list of sentence strings'''\n",
    "    doc = nlp(' '.join(text.tolist()))\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    return([str(sent) for sent in doc.sents])\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    '''input: string or pandas series of text\n",
    "       returns: list of all words'''\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    return(' '.join(text).split())\n",
    "\n",
    "def syllable_count(word):\n",
    "    '''counts number of syllables in a word'''\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def count_total_words(text):\n",
    "    '''count total number of words in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer'''\n",
    "    if isinstance(text, list):\n",
    "        list_of_sentence = text\n",
    "    elif isinstance(text, str):\n",
    "        list_of_sentence = [text]\n",
    "    elif isinstance(text, pd.Series):\n",
    "        list_of_sentence = text_to_sentence(text)\n",
    "    else:\n",
    "        print('count_total_words: Error: not a string or pandas series object.')\n",
    "    list_of_word_count = [words_per_sentence(str(sentence)) for sentence in list_of_sentence]\n",
    "    return(np.sum(list_of_word_count))\n",
    "\n",
    "def count_total_sentences(text):\n",
    "    '''count total number of sentences in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return(len(sentences))\n",
    "\n",
    "# Count Syllables\n",
    "# https://datascience.stackexchange.com/questions/23376/how-to-get-the-number-of-syllables-in-a-word\n",
    "def syllables(word):\n",
    "    '''backup syllable counter if word not in NLTK-CMU dictionary'''\n",
    "    #referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        if word[0] in vowels:\n",
    "            count +=1\n",
    "    except:\n",
    "        count += 0\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "d = cmudict.dict()\n",
    "def nsyl(word):\n",
    "    '''input: string - word\n",
    "       returns: integer count of syllables in word'''\n",
    "    try:\n",
    "        # needs the [0] otherwise words like 'of' returns [1,1]\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)\n",
    "\n",
    "def count_total_syllables(text):\n",
    "    '''count total number of sentences in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count'''\n",
    "    if isinstance(text, str):\n",
    "        list_of_sentence = [text]\n",
    "    elif isinstance(text, pd.Series):\n",
    "        list_of_sentence = text_to_sentence(text)\n",
    "    else:\n",
    "        print('count_total_syllables: Error: not a string or pandas series object.')\n",
    "    list_of_words = text_to_wordlist(list_of_sentence)\n",
    "    syllable_list = [nsyl(word) for word in list_of_words]\n",
    "    return(np.sum(syllable_list))\n",
    "\n",
    "def count_of_letters(text):\n",
    "    '''count total number of letters or digits in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    # Replace punctuations with an empty string.\n",
    "    str1 = re.sub(r\"[^\\w\\s]|_\", \"\", text)\n",
    "    no_spaces = str1.replace(\" \", \"\")\n",
    "    return(len(no_spaces))\n",
    "\n",
    "\n",
    "def difficult_words_list(list1):\n",
    "    '''returns difference of list with easy_word list for Dale-Chall\n",
    "       input: two lists of strings/words\n",
    "       returns: list of unique words in both lists'''\n",
    "    if isinstance(list1, pd.Series):\n",
    "        list1 = ' '.join(text)\n",
    "    if isinstance(list1, list):\n",
    "        list1 = list1[0]\n",
    "    try:\n",
    "        easy_words_file = open('./Data/word_lists/DaleChallEasyWordList.txt', 'r')\n",
    "        easy_words = easy_words_file.read().split('\\n')\n",
    "    except E:\n",
    "        print(\"Error reading easy words file\", E)\n",
    "    easy_words_file.close()\n",
    "    easy_words = [word.lower() for word in easy_words]\n",
    "    easy_words = set(easy_words)\n",
    "    diff = [word.lower() for word in list1.split() if word.lower() not in easy_words]\n",
    "    return(diff)\n",
    "\n",
    "def dc_difficult_word_count(text):\n",
    "    '''Count of difficult words - those not in Dale-Chall Easy Word List\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count of difficult words in text'''\n",
    "    list_of_dc_difficult = difficult_words_list(text)\n",
    "    return(len(list_of_dc_difficult))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def gf_complex_word_count(text):\n",
    "    '''Count of complex - >= 3 syllables with caveats\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count of complex words in text'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    if isinstance(text, list):\n",
    "        text = str(text[0])\n",
    "    text = [word.lower() for word in text.split()]\n",
    "    lemma = [lemmatizer.lemmatize(word) for word in text]\n",
    "    stem = [re.sub(\"(?:ing|ed|es|ly)$\",\"\",word) for word in text]\n",
    "    syllable_list = [nsyl(word) for word in stem]\n",
    "    complex_count = sum(x > 2 for x in syllable_list)\n",
    "    return(complex_count)\n",
    "\n",
    "def smog_poly_count(text):\n",
    "    '''counts number of words with 3 or more syllables\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count of polysyllabic words in text'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    if isinstance(text, list):\n",
    "        text = str(text[0])\n",
    "    text = [word.lower() for word in text.split()]\n",
    "    syllable_list = [nsyl(word) for word in text]\n",
    "    poly_count = sum(x > 2 for x in syllable_list)\n",
    "    return(poly_count)\n",
    "\n",
    "\n",
    "######################## READABILITY SCORES ########################\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Automated_readability_index\n",
    "# 4.71(chars/word) + 0.5(words/sentence) - 21.43\n",
    "def ari(text):\n",
    "    '''input: string of sentence\n",
    "       returns: float ari score'''\n",
    "    character_count = count_of_letters(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    ari = 4.71*(character_count/word_count) + 0.5*(word_count/sentence_count) - 21.43\n",
    "    return(ari)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\n",
    "# Flesch–Kincaid grade level\n",
    "# 0.39(total words/total sentences) + 11.8(total syllables/total words) - 15.59\n",
    "def flesch_kincaid(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - flesh kincaid grade level score'''\n",
    "    num_total_words = count_total_words(text)\n",
    "    num_total_sentences = count_total_sentences(text)\n",
    "    num_total_syllables = count_total_syllables(text)\n",
    "    fkgl = 0.39*(num_total_words/num_total_sentences) + 11.8*(num_total_syllables/num_total_words) - 15.59\n",
    "    return(fkgl)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\n",
    "# 0.0588(average number of letters per 100 words) - 0.296(average number of sentences per 100 words) - 15.8\n",
    "def coleman_liau(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - Coleman-Liau index'''\n",
    "    character_count = count_of_letters(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    l = character_count/word_count*100\n",
    "    s = sentence_count/word_count*100\n",
    "    cl = 0.0588*l - 0.296*s - 15.8\n",
    "    return(cl)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula\n",
    "# 0.1579(100*difficult words/words) + 0.496(words/sentences)\n",
    "def dale_chall(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - Dale-Chall readability score'''\n",
    "    difficult_words = dc_difficult_word_count(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    dc = 0.1579*(100*difficult_words/word_count) + 0.496*(word_count/sentence_count)\n",
    "    return(dc)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Gunning_fog_index\n",
    "# 0.4[(words/sentence) + 100(complex words/words)]\n",
    "def gunning_fog(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - Gunning Fog index readability score'''\n",
    "    complex_words = gf_complex_word_count(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    gf = 0.4*((word_count/sentence_count) + 100*(complex_words/word_count))\n",
    "    return(gf)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/SMOG\n",
    "# 1.043*sqrt(30*number polysylables/number sentences)+3.1291\n",
    "def smog(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - SMOG grade readability score'''\n",
    "    poly_count = smog_poly_count(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    smog_score = 1.043*np.sqrt(30*poly_count/sentence_count) + 3.1291\n",
    "    return(smog_score)\n",
    "\n",
    "text_df['char_count'] = text_df['text'].apply(count_of_letters)\n",
    "text_df['syl_count'] = text_df['text'].apply(count_total_syllables)\n",
    "text_df['word_count'] = text_df['text'].apply(count_total_words)\n",
    "text_df['char_per_word'] = text_df['char_count']/text_df['word_count']#\n",
    "text_df['syl_per_word'] = text_df['syl_count']/text_df['word_count']#\n",
    "text_df['sent_count'] = text_df['text'].apply(count_total_sentences)\n",
    "text_df['word_per_sent'] = text_df['word_count']/text_df['sent_count']#\n",
    "\n",
    "text_df['dc_word_count'] = text_df['text'].apply(dc_difficult_word_count)\n",
    "text_df['gf_word_count'] = text_df['text'].apply(gf_complex_word_count)\n",
    "text_df['poly_word_count'] = text_df['text'].apply(smog_poly_count)\n",
    "\n",
    "text_df['dc_word_perc'] = text_df['dc_word_count']/text_df['word_count']#\n",
    "text_df['gf_word_perc'] = text_df['gf_word_count']/text_df['word_count']#\n",
    "text_df['poly_word_perc'] = text_df['poly_word_count']/text_df['word_count']#\n",
    "\n",
    "text_df['ari'] = text_df['text'].apply(ari)\n",
    "text_df['flesch_kincaid'] = text_df['text'].apply(flesch_kincaid)\n",
    "text_df['coleman_liau'] = text_df['text'].apply(coleman_liau)\n",
    "text_df['dale_chall'] = text_df['text'].apply(dale_chall)\n",
    "text_df['gunning_fog'] = text_df['text'].apply(gunning_fog)\n",
    "text_df['smog'] = text_df['text'].apply(smog)\n",
    "print(\"text_df.shape = \", text_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ca61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------PARALLEL PHRASE COUNT---------\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n",
      "Be sure Stanford CoreNLP server started and parser instantiated!\n",
      "Errors also seem to occur with quotes\n"
     ]
    }
   ],
   "source": [
    "print('----------PARALLEL PHRASE COUNT---------')\n",
    "parser = CoreNLPParser()\n",
    "def count_parallels(sents):\n",
    "    count = 0\n",
    "    print_error = True\n",
    "    for phrase in sents:\n",
    "        try:\n",
    "            sent, = parser.parse_text(phrase)\n",
    "        except:\n",
    "            if print_error:\n",
    "                #print(\"Be sure Stanford CoreNLP server started and parser instantiated!\")\n",
    "                #print(\"Errors also seem to occur with quotes\")\n",
    "                print_error = False\n",
    "            continue\n",
    "        poss = []\n",
    "        words = []\n",
    "        for word in sent.pos():\n",
    "            poss.append(word[1])\n",
    "            words.append(word[0])\n",
    "        stop = False\n",
    "        results = []\n",
    "        for length in range(7,3,-1):\n",
    "            length = min(length, len(words))\n",
    "            for i in range(len(poss)-length+1):\n",
    "                for j in range(len(poss)-length+1):\n",
    "                    if abs(i-j) > length:\n",
    "                        if poss[i:i+length]==poss[j:j+length]:\n",
    "                            if length > 4 or (',' not in poss[i:i+length] and '``' not in poss[i:i+length]):\n",
    "                                results.append([i,j,length])\n",
    "                                count += 1\n",
    "                                stop = True\n",
    "                                break\n",
    "                if stop: break\n",
    "            if stop: break\n",
    "\n",
    "    return(count)\n",
    "\n",
    "text_df['parallel_count'] = text_df.text.apply(count_parallels)\n",
    "text_df['parallel_per_sent'] = text_df.parallel_count/text_df.num_sents\n",
    "print(\"text_df.shape = \", text_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f514132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------PARSE TREE DEPTH----------')\n",
    "\n",
    "# Make data frame of sentences and parse tree depth of each\n",
    "def walk_tree_depth(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return max(walk_tree_depth(child, depth+1) for child in node.children )\n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "tree_depth = pd.DataFrame(columns = ['date', 'source', 'sentence', 'depth', 'file'])\n",
    "for i, speech in enumerate(text_df['text']):\n",
    "    for j, sentence in enumerate(speech):\n",
    "        doc = nlp_spacy(sentence)\n",
    "        depth = [walk_tree_depth(sent.root, 0) for sent in doc.sents][0]\n",
    "        tree_depth.loc[len(tree_depth)] = [text_df.date[i], text_df['source'].iloc[i], sentence, depth, text_df.index[i]]\n",
    "\n",
    "mean_depth=tree_depth.groupby(by='date').mean(depth)\n",
    "text_df=pd.merge(text_df, mean_depth, how='left', on='file')\n",
    "tree_depth.to_csv('./Data/genData/sentence_depth_'+suffix+'_sentences.csv',index=False)\n",
    "print(\"text_df.shape = \", text_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('./Data/genData/tidy_data_amrhets_sentence_partial_depth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6beaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
