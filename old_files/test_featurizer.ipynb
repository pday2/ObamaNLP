{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7f9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dependency parse tree depth\n",
    "# Using Stanford's CoreNLP parser with NLTK\n",
    "# 1. Download CoreNLP from https://stanfordnlp.github.io/CoreNLP/download.html\n",
    "# 2. make sure Java is installed, otherwise download and install Java - https://www.java.com/en/download/windows_manual.jsp\n",
    "# 3. Unzip/extract CoreNLP zip file to a directory\n",
    "# 4. Go to that directory and open a command terminal, and run the following command...\n",
    "# 4b. on my laptop its in C:\\Users\\peter\\stanford-corenlp-4.5.2\n",
    "# 5. java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "# 6. Now for graphviz if you want to view the parse trees, download from https://graphviz.org/download/ then install\n",
    "# 7. Now, can run the following python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671cad12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7cbcebae4a4f1dad0a50f8c5e89220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 17:34:04 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-28 17:34:05 INFO: File exists: C:\\Users\\peter\\stanza_resources\\en\\default.zip\n",
      "2023-04-28 17:34:09 INFO: Finished downloading models and saved to C:\\Users\\peter\\stanza_resources.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peter\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import regex as re\n",
    "import os\n",
    "import sys\n",
    "import unicodedata\n",
    "import re\n",
    "import stanza\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import cmudict\n",
    "stanza.download('en')\n",
    "nltk.download('punkt')\n",
    "word_token = TreebankWordTokenizer()\n",
    "from nrclex import NRCLex\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download ('wordnet')\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c680b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c79b823e464e0a9659fc64d1b5ca83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 08:35:52 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-29 08:35:54 INFO: File exists: C:\\Users\\peter\\stanza_resources\\en\\default.zip\n",
      "2023-04-29 08:35:58 INFO: Finished downloading models and saved to C:\\Users\\peter\\stanza_resources.\n",
      "2023-04-29 08:35:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135ecd5385654eb386f17aa6d7854452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 08:35:58 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-04-29 08:35:58 INFO: Use device: gpu\n",
      "2023-04-29 08:35:58 INFO: Loading: tokenize\n",
      "2023-04-29 08:35:59 INFO: Loading: pos\n",
      "2023-04-29 08:35:59 INFO: Loading: lemma\n",
      "2023-04-29 08:35:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en', processors='tokenize, lemma, pos')\n",
    "nlp_spacy = spacy.load(\"en_core_web_md\")\n",
    "stopwords = pd.read_table('./Data/word_lists/kaggle_stopwords.txt')\n",
    "paths = ['./Data/amrhet/']\n",
    "suffix = 'amrhet'\n",
    "source = 'amrhet'\n",
    "dates_file = 'datetitle.csv'\n",
    "#paths = ['./Data/amrhet/']\n",
    "#suffix = 'amrhet'\n",
    "#source = suffix\n",
    "#dates_file = 'datetitle.csv'\n",
    "dates = pd.read_csv(dates_file)\n",
    "dates['date'] = pd.to_datetime(dates['date'], format='%Y-%m-%d')\n",
    "# date, title, file\n",
    "try:\n",
    "    dates.rename(columns={\"url\":\"file\"}, inplace=True)\n",
    "except:\n",
    "    print()\n",
    "try:\n",
    "    dates = dates.drop('title', axis=1)\n",
    "except:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cc3c2ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------LOADING DOCUMENTS----------\n",
      "Length of text_df: 433\n"
     ]
    }
   ],
   "source": [
    "print('---------LOADING DOCUMENTS----------')\n",
    "# Load up the speeches\n",
    "speeches = []\n",
    "for path in paths:\n",
    "    list_of_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                list_of_files.append(os.path.join(root,file))\n",
    "\n",
    "    for file in list_of_files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        speeches.append(text)\n",
    "\n",
    "#clean out goofy unicode  space characters \n",
    "speeches = [unicodedata.normalize(\"NFKD\", speech) for speech in speeches if len(speech)>0 ]\n",
    "#clean out xa0 space characters\n",
    "[speech.replace(u'\\xa0', '') for speech in speeches]; # ; supresses output\n",
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "speeches = [remove_bracket(speech) for speech in speeches]\n",
    "# Clean up whitespace\n",
    "speeches = [re.sub('[\\s+]', ' ', speech) for speech in speeches]\n",
    "# Remove -- that's all over the amrhet files\n",
    "def remove_dashes(text):\n",
    "    return re.sub(r'-- ', '', text)\n",
    "speeches = [remove_dashes(speech) for speech in speeches]\n",
    "text_df = pd.DataFrame({'file' : list_of_files,\n",
    "                        'text' : speeches})\n",
    "\n",
    "text_df = pd.merge(text_df, dates, how='inner', on='file')\n",
    "text_df = text_df.sort_values(by='date', ignore_index=True)\n",
    "text_df = text_df[['date', 'file', 'text']]\n",
    "text_df['source'] = 'oba'\n",
    "text_df.set_index('file', inplace=True)\n",
    "text_df['sentences'] = text_df['text'].apply(sent_tokenize)\n",
    "text_df['words'] = text_df['text'].apply(word_token.tokenize)\n",
    "text_df['word_set'] = text_df['words'].apply(set)\n",
    "text_df['num_sents'] = text_df['sentences'].apply(len)\n",
    "text_df['num_words'] = text_df['words'].apply(len)\n",
    "text_df['num_unique_words'] = text_df['word_set'].apply(len)\n",
    "print(\"Length of text_df:\", len(text_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "778f13f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------POS TAGGING---------\n",
      "Length of text_df: 433\n",
      "...pst, this is slow\n",
      "0 NUM 0.0009478672985781991\n",
      "0 ADV 0.024644549763033177\n",
      "0 SYM 0.0\n",
      "0 NOUN 0.1895734597156398\n",
      "0 ADP 0.11374407582938388\n",
      "0 PROPN 0.046445497630331754\n",
      "0 DET 0.0995260663507109\n",
      "0 INTJ 0.0\n",
      "0 AUX 0.04549763033175355\n",
      "0 CCONJ 0.04928909952606635\n",
      "0 ADJ 0.07203791469194312\n",
      "0 PRON 0.0976303317535545\n",
      "0 SCONJ 0.01990521327014218\n",
      "0 X 0.0\n",
      "0 VERB 0.10900473933649289\n",
      "0 PUNCT 0.10710900473933649\n",
      "0 PART 0.024644549763033177\n",
      "40 NUM 0.003927729772191673\n",
      "40 ADV 0.03534956794972506\n",
      "40 SYM 0.0\n",
      "40 NOUN 0.16496465043205027\n",
      "40 ADP 0.09478921183555905\n",
      "40 PROPN 0.05943964388583399\n",
      "40 DET 0.0945273631840796\n",
      "40 INTJ 0.0005236973029588898\n",
      "40 AUX 0.061534433097669546\n",
      "40 CCONJ 0.05027494108405342\n",
      "40 ADJ 0.07462686567164178\n",
      "40 PRON 0.09714584969887405\n",
      "40 SCONJ 0.02251898402723226\n",
      "40 X 0.0\n",
      "40 VERB 0.11442786069651742\n",
      "40 PUNCT 0.09583660644147683\n",
      "40 PART 0.03011259492013616\n",
      "80 NUM 0.012147195426938193\n",
      "80 ADV 0.0420388233893057\n",
      "80 SYM 0.0003572704537334762\n",
      "80 NOUN 0.18339883291651782\n",
      "80 ADP 0.08312492556865547\n",
      "80 PROPN 0.0285816362986781\n",
      "80 DET 0.07943313088007621\n",
      "80 INTJ 0.0009527212099559367\n",
      "80 AUX 0.06347505061331428\n",
      "80 CCONJ 0.03798975824699297\n",
      "80 ADJ 0.06633321424318209\n",
      "80 PRON 0.11015838990115517\n",
      "80 SCONJ 0.027271644634988688\n",
      "80 X 0.00011909015124449208\n",
      "80 VERB 0.1306418959152078\n",
      "80 PUNCT 0.10396570203644159\n",
      "80 PART 0.030010718113612004\n",
      "120 NUM 0.009259259259259259\n",
      "120 ADV 0.03418803418803419\n",
      "120 SYM 0.0\n",
      "120 NOUN 0.17592592592592593\n",
      "120 ADP 0.1168091168091168\n",
      "120 PROPN 0.0633903133903134\n",
      "120 DET 0.08404558404558404\n",
      "120 INTJ 0.002136752136752137\n",
      "120 AUX 0.054843304843304845\n",
      "120 CCONJ 0.046296296296296294\n",
      "120 ADJ 0.06054131054131054\n",
      "120 PRON 0.1047008547008547\n",
      "120 SCONJ 0.021367521367521368\n",
      "120 X 0.0\n",
      "120 VERB 0.09971509971509972\n",
      "120 PUNCT 0.10398860398860399\n",
      "120 PART 0.022792022792022793\n",
      "160 NUM 0.007204610951008645\n",
      "160 ADV 0.056195965417867436\n",
      "160 SYM 0.0\n",
      "160 NOUN 0.19452449567723343\n",
      "160 ADP 0.0792507204610951\n",
      "160 PROPN 0.02377521613832853\n",
      "160 DET 0.069164265129683\n",
      "160 INTJ 0.0007204610951008645\n",
      "160 AUX 0.069164265129683\n",
      "160 CCONJ 0.04755043227665706\n",
      "160 ADJ 0.05187319884726225\n",
      "160 PRON 0.11311239193083573\n",
      "160 SCONJ 0.02881844380403458\n",
      "160 X 0.0\n",
      "160 VERB 0.14697406340057637\n",
      "160 PUNCT 0.07348703170028818\n",
      "160 PART 0.03818443804034582\n",
      "200 NUM 0.0055762081784386614\n",
      "200 ADV 0.03903345724907063\n",
      "200 SYM 0.0\n",
      "200 NOUN 0.16171003717472118\n",
      "200 ADP 0.09851301115241635\n",
      "200 PROPN 0.03903345724907063\n",
      "200 DET 0.08921933085501858\n",
      "200 INTJ 0.0\n",
      "200 AUX 0.05762081784386617\n",
      "200 CCONJ 0.05204460966542751\n",
      "200 ADJ 0.046468401486988845\n",
      "200 PRON 0.15055762081784388\n",
      "200 SCONJ 0.022304832713754646\n",
      "200 X 0.0\n",
      "200 VERB 0.11895910780669144\n",
      "200 PUNCT 0.09293680297397769\n",
      "200 PART 0.026022304832713755\n",
      "240 NUM 0.0073041168658698535\n",
      "240 ADV 0.041168658698539175\n",
      "240 SYM 0.0\n",
      "240 NOUN 0.19123505976095617\n",
      "240 ADP 0.09296148738379814\n",
      "240 PROPN 0.024568393094289508\n",
      "240 DET 0.08233731739707835\n",
      "240 INTJ 0.0006640106241699867\n",
      "240 AUX 0.05644090305444887\n",
      "240 CCONJ 0.041168658698539175\n",
      "240 ADJ 0.0796812749003984\n",
      "240 PRON 0.09893758300132802\n",
      "240 SCONJ 0.034528552456839307\n",
      "240 X 0.0\n",
      "240 VERB 0.12151394422310757\n",
      "240 PUNCT 0.08897742363877822\n",
      "240 PART 0.03851261620185923\n",
      "280 NUM 0.006619593998234775\n",
      "280 ADV 0.02912621359223301\n",
      "280 SYM 0.0\n",
      "280 NOUN 0.18358340688437777\n",
      "280 ADP 0.09929390997352162\n",
      "280 PROPN 0.05913503971756399\n",
      "280 DET 0.07281553398058252\n",
      "280 INTJ 0.0\n",
      "280 AUX 0.05648720211827008\n",
      "280 CCONJ 0.04457193292144748\n",
      "280 ADJ 0.07546337157987644\n",
      "280 PRON 0.10194174757281553\n",
      "280 SCONJ 0.018534863195057368\n",
      "280 X 0.0\n",
      "280 VERB 0.11562224183583407\n",
      "280 PUNCT 0.10723742277140336\n",
      "280 PART 0.029567519858781994\n",
      "320 NUM 0.014481707317073171\n",
      "320 ADV 0.039634146341463415\n",
      "320 SYM 0.0007621951219512195\n",
      "320 NOUN 0.17911585365853658\n",
      "320 ADP 0.10746951219512195\n",
      "320 PROPN 0.07240853658536585\n",
      "320 DET 0.07164634146341463\n",
      "320 INTJ 0.0\n",
      "320 AUX 0.038871951219512195\n",
      "320 CCONJ 0.038109756097560975\n",
      "320 ADJ 0.08155487804878049\n",
      "320 PRON 0.07240853658536585\n",
      "320 SCONJ 0.027439024390243903\n",
      "320 X 0.0\n",
      "320 VERB 0.12423780487804878\n",
      "320 PUNCT 0.09070121951219512\n",
      "320 PART 0.041158536585365856\n",
      "360 NUM 0.00658165364047717\n",
      "360 ADV 0.0427807486631016\n",
      "360 SYM 0.0\n",
      "360 NOUN 0.15220074043603454\n",
      "360 ADP 0.09749074454956808\n",
      "360 PROPN 0.04895104895104895\n",
      "360 DET 0.07157548334018922\n",
      "360 INTJ 0.0008227067050596463\n",
      "360 AUX 0.059646236116824354\n",
      "360 CCONJ 0.0427807486631016\n",
      "360 ADJ 0.0658165364047717\n",
      "360 PRON 0.12381735911147676\n",
      "360 SCONJ 0.027149321266968326\n",
      "360 X 0.0\n",
      "360 VERB 0.10160427807486631\n",
      "360 PUNCT 0.12793089263677498\n",
      "360 PART 0.030851501439736733\n",
      "400 NUM 0.007577737130911941\n",
      "400 ADV 0.045466422785471645\n",
      "400 SYM 0.0\n",
      "400 NOUN 0.1599163835902796\n",
      "400 ADP 0.08152599947739744\n",
      "400 PROPN 0.03710478181343089\n",
      "400 DET 0.07107394826234649\n",
      "400 INTJ 0.0010452051215050953\n",
      "400 AUX 0.06637052521557356\n",
      "400 CCONJ 0.0459890253462242\n",
      "400 ADJ 0.058008884243532795\n",
      "400 PRON 0.12333420433760126\n",
      "400 SCONJ 0.032662660047034234\n",
      "400 X 0.0\n",
      "400 VERB 0.12856022994512672\n",
      "400 PUNCT 0.10661092239351973\n",
      "400 PART 0.03475307029004442\n",
      "Length of text_df: 433\n",
      "----------------------DONE!!!!!!!!!!!!-------------------\n"
     ]
    }
   ],
   "source": [
    "############# POS TAGGING ###################  NEW TRY!!!!\n",
    "print('---------POS TAGGING---------')\n",
    "\n",
    "# The nlp(text) uses a lot of gpu memory, causes errors sometimes, may need to restart notebook to freshen things up\n",
    "parts_of_speech = ['NUM','ADV','SYM','NOUN','ADP','PROPN','DET','INTJ','AUX',\n",
    "                   'CCONJ','ADJ','PRON','SCONJ','X','VERB','PUNCT','PART']\n",
    "for col in parts_of_speech:\n",
    "    text_df[col] = 0\n",
    "print(\"Length of text_df:\", len(text_df))\n",
    "print('...pst, this is slow')\n",
    "for i, text in enumerate(text_df.text):\n",
    "    doc = nlp_stanza(text) # Run stanza on each speech\n",
    "    mat_of_pos = [[word.pos for word in sentence.words] for sentence in doc.sentences] # matrix of POS for each sentence\n",
    "    # How to flatten a list = [item for sublist in list_of_lists for item in sublist]\n",
    "    list_of_pos = [pos for sentence in mat_of_pos for pos in sentence] # flatten matrix into one list of all pos\n",
    "    total_pos_count = len(list_of_pos)\n",
    "    for pos in parts_of_speech:\n",
    "        #dfd.iloc[[0, 2], dfd.columns.get_loc('A')]\n",
    "        text_df.iloc[i, text_df.columns.get_loc(pos)] = list_of_pos.count(pos)/total_pos_count\n",
    "        #text_df.at[i, pos] = list_of_pos.count(pos)/total_pos_count\n",
    "        if i%40==0: print(i, pos, list_of_pos.count(pos)/total_pos_count)\n",
    "        \n",
    "\n",
    "text_df = text_df.dropna()\n",
    "print(\"Length of text_df:\", len(text_df))\n",
    "print(\"----------------------DONE!!!!!!!!!!!!-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things have changed, this may not work\n",
    "print('---POS Long-facet data---')\n",
    "# Reshape POS data to be long for facet plots\n",
    "POSFacetPlotData  = pd.DataFrame(columns=['date', 'file', 'source', 'pos', 'proportion'])\n",
    "for i in range(len(text_df)):\n",
    "    for pos in unique_pos:\n",
    "        POSFacetPlotData.loc[len(POSFacetPlotData)] = [text_df.date[i], text_df.index[i], source, pos, text_df[pos].iloc[i]]            \n",
    "POSFacetPlotData['date'] = pd.to_datetime(POSFacetPlotData['date'])\n",
    "POSFacetPlotData.to_csv('./Data/genData/pos_long_data'+suffix+'.csv', index=False)\n",
    "\n",
    "print(\"Length of text_df:\", len(text_df))\n",
    "print('DONE!!!!!!!!!!!!!!!!!!!!!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbf6cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------NRCLex EMOTION TAGGING---------\n"
     ]
    }
   ],
   "source": [
    "print('---------NRCLex EMOTION TAGGING---------')\n",
    "text_df.dropna(inplace=True)\n",
    "text_df['emo'] = text_df.text.apply(NRCLex)\n",
    "\n",
    "# Get names of emotion attributes, locate and remove anticipation as it seems to alway be 0\n",
    "emotions = {'fear': 0.077,\n",
    "            'anger': 0.019,\n",
    "            'anticip': 0.0,\n",
    "            'trust': 0.203,\n",
    "            'surprise': 0.019,\n",
    "            'positive': 0.330,\n",
    "            'negative': 0.106,\n",
    "            'sadness': 0.067,\n",
    "            'disgust': 0.019,\n",
    "            'joy': 0.087,\n",
    "            'anticipation': 0.067}\n",
    "Attributes = list(emotions.keys())\n",
    "antIndx = Attributes.index('anticip')\n",
    "#AttNo = len(emotions[0].affect_frequencies.keys())-1\n",
    "\n",
    "# Make a column for emo values for each source and each emotion\n",
    "# We could also try correlation between each of these columns and the econ/approval data\n",
    "\n",
    "# use list() around the affect_frequencies.values() to get numbers in list form\n",
    "# starting point: [list(emotion.affect_frequencies.values())[1] for emotion in df['emo_oba']]\n",
    "indexes = [0,1,3,4,5,6,7,8,9,10] # skip 2 which is anticipation and seems to always be 0\n",
    "for i, attr in enumerate(Attributes):\n",
    "    if not(i==2):\n",
    "        text_df[attr]=[list(emotion.affect_frequencies.values())[i] for emotion in text_df['emo']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------TextBlob----------')\n",
    "text_df['TBsubjectivity']=[TextBlob(text).sentiment.subjectivity for text in text_df['text']]\n",
    "text_df['TBpolarity']=[TextBlob(text).sentiment.polarity for text in text_df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bcf549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------READABILITY----------')\n",
    "\n",
    "########## HELPER FUNCTIONS #############\n",
    "def words_per_sentence(sentence):\n",
    "    '''returns: integer number of words in a sentence'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return(len(tokens))\n",
    "\n",
    "def chars_per_word(word):\n",
    "    '''returns: integer number of characters in a word'''\n",
    "    return(len(word))\n",
    "\n",
    "def string_to_list(sentence):\n",
    "    '''converts a string/sentence to a list of words'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return(tokenizer.tokenize(sentence))\n",
    "\n",
    "def chars_per_word_sentence(sentence):\n",
    "    '''input: string of a sentence\n",
    "       returns: list of number of characters in a sentence'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    char_len_list = [chars_per_word(word) for word in tokens]\n",
    "    return(char_len_list)\n",
    "\n",
    "def text_to_sentence(text):\n",
    "    '''uses spacy nlp object to break up sentences\n",
    "       input: pandas series of strings\n",
    "       returns: list of sentence strings'''\n",
    "    doc = nlp(' '.join(text.tolist()))\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    return([str(sent) for sent in doc.sents])\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    '''input: string or pandas series of text\n",
    "       returns: list of all words'''\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    return(' '.join(text).split())\n",
    "\n",
    "def syllable_count(word):\n",
    "    '''counts number of syllables in a word'''\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def count_total_words(text):\n",
    "    '''count total number of words in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer'''\n",
    "    if isinstance(text, list):\n",
    "        list_of_sentence = text\n",
    "    elif isinstance(text, str):\n",
    "        list_of_sentence = [text]\n",
    "    elif isinstance(text, pd.Series):\n",
    "        list_of_sentence = text_to_sentence(text)\n",
    "    else:\n",
    "        print('count_total_words: Error: not a string or pandas series object.')\n",
    "    list_of_word_count = [words_per_sentence(str(sentence)) for sentence in list_of_sentence]\n",
    "    return(np.sum(list_of_word_count))\n",
    "\n",
    "def count_total_sentences(text):\n",
    "    '''count total number of sentences in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return(len(sentences))\n",
    "\n",
    "# Count Syllables\n",
    "# https://datascience.stackexchange.com/questions/23376/how-to-get-the-number-of-syllables-in-a-word\n",
    "def syllables(word):\n",
    "    '''backup syllable counter if word not in NLTK-CMU dictionary'''\n",
    "    #referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        if word[0] in vowels:\n",
    "            count +=1\n",
    "    except:\n",
    "        count += 0\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "d = cmudict.dict()\n",
    "def nsyl(word):\n",
    "    '''input: string - word\n",
    "       returns: integer count of syllables in word'''\n",
    "    try:\n",
    "        # needs the [0] otherwise words like 'of' returns [1,1]\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)\n",
    "\n",
    "def count_total_syllables(text):\n",
    "    '''count total number of sentences in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count'''\n",
    "    if isinstance(text, str):\n",
    "        list_of_sentence = [text]\n",
    "    elif isinstance(text, pd.Series):\n",
    "        list_of_sentence = text_to_sentence(text)\n",
    "    else:\n",
    "        print('count_total_syllables: Error: not a string or pandas series object.')\n",
    "    list_of_words = text_to_wordlist(list_of_sentence)\n",
    "    syllable_list = [nsyl(word) for word in list_of_words]\n",
    "    return(np.sum(syllable_list))\n",
    "\n",
    "def count_of_letters(text):\n",
    "    '''count total number of letters or digits in a text\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    # Replace punctuations with an empty string.\n",
    "    str1 = re.sub(r\"[^\\w\\s]|_\", \"\", text)\n",
    "    no_spaces = str1.replace(\" \", \"\")\n",
    "    return(len(no_spaces))\n",
    "\n",
    "\n",
    "def difficult_words_list(list1):\n",
    "    '''returns difference of list with easy_word list for Dale-Chall\n",
    "       input: two lists of strings/words\n",
    "       returns: list of unique words in both lists'''\n",
    "    if isinstance(list1, pd.Series):\n",
    "        list1 = ' '.join(text)\n",
    "    if isinstance(list1, list):\n",
    "        list1 = list1[0]\n",
    "    try:\n",
    "        easy_words_file = open('./Data/word_lists/DaleChallEasyWordList.txt', 'r')\n",
    "        easy_words = easy_words_file.read().split('\\n')\n",
    "    except E:\n",
    "        print(\"Error reading easy words file\", E)\n",
    "    easy_words_file.close()\n",
    "    easy_words = [word.lower() for word in easy_words]\n",
    "    easy_words = set(easy_words)\n",
    "    diff = [word.lower() for word in list1.split() if word.lower() not in easy_words]\n",
    "    return(diff)\n",
    "\n",
    "def dc_difficult_word_count(text):\n",
    "    '''Count of difficult words - those not in Dale-Chall Easy Word List\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count of difficult words in text'''\n",
    "    list_of_dc_difficult = difficult_words_list(text)\n",
    "    return(len(list_of_dc_difficult))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def gf_complex_word_count(text):\n",
    "    '''Count of complex - >= 3 syllables with caveats\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count of complex words in text'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    if isinstance(text, list):\n",
    "        text = str(text[0])\n",
    "    text = [word.lower() for word in text.split()]\n",
    "    lemma = [lemmatizer.lemmatize(word) for word in text]\n",
    "    stem = [re.sub(\"(?:ing|ed|es|ly)$\",\"\",word) for word in text]\n",
    "    syllable_list = [nsyl(word) for word in stem]\n",
    "    complex_count = sum(x > 2 for x in syllable_list)\n",
    "    return(complex_count)\n",
    "\n",
    "def smog_poly_count(text):\n",
    "    '''counts number of words with 3 or more syllables\n",
    "       input: str or pandas.core.series.Series of text\n",
    "       returns: integer count of polysyllabic words in text'''\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(text)\n",
    "    if isinstance(text, list):\n",
    "        text = str(text[0])\n",
    "    text = [word.lower() for word in text.split()]\n",
    "    syllable_list = [nsyl(word) for word in text]\n",
    "    poly_count = sum(x > 2 for x in syllable_list)\n",
    "    return(poly_count)\n",
    "\n",
    "\n",
    "######################## READABILITY SCORES ########################\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Automated_readability_index\n",
    "# 4.71(chars/word) + 0.5(words/sentence) - 21.43\n",
    "def ari(text):\n",
    "    '''input: string of sentence\n",
    "       returns: float ari score'''\n",
    "    character_count = count_of_letters(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    ari = 4.71*(character_count/word_count) + 0.5*(word_count/sentence_count) - 21.43\n",
    "    return(ari)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\n",
    "# Flesch–Kincaid grade level\n",
    "# 0.39(total words/total sentences) + 11.8(total syllables/total words) - 15.59\n",
    "def flesch_kincaid(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - flesh kincaid grade level score'''\n",
    "    num_total_words = count_total_words(text)\n",
    "    num_total_sentences = count_total_sentences(text)\n",
    "    num_total_syllables = count_total_syllables(text)\n",
    "    fkgl = 0.39*(num_total_words/num_total_sentences) + 11.8*(num_total_syllables/num_total_words) - 15.59\n",
    "    return(fkgl)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\n",
    "# 0.0588(average number of letters per 100 words) - 0.296(average number of sentences per 100 words) - 15.8\n",
    "def coleman_liau(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - Coleman-Liau index'''\n",
    "    character_count = count_of_letters(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    l = character_count/word_count*100\n",
    "    s = sentence_count/word_count*100\n",
    "    cl = 0.0588*l - 0.296*s - 15.8\n",
    "    return(cl)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula\n",
    "# 0.1579(100*difficult words/words) + 0.496(words/sentences)\n",
    "def dale_chall(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - Dale-Chall readability score'''\n",
    "    difficult_words = dc_difficult_word_count(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    dc = 0.1579*(100*difficult_words/word_count) + 0.496*(word_count/sentence_count)\n",
    "    return(dc)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Gunning_fog_index\n",
    "# 0.4[(words/sentence) + 100(complex words/words)]\n",
    "def gunning_fog(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - Gunning Fog index readability score'''\n",
    "    complex_words = gf_complex_word_count(text)\n",
    "    word_count = count_total_words(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    gf = 0.4*((word_count/sentence_count) + 100*(complex_words/word_count))\n",
    "    return(gf)\n",
    "\n",
    "# https://en.wikipedia.org/wiki/SMOG\n",
    "# 1.043*sqrt(30*number polysylables/number sentences)+3.1291\n",
    "def smog(text):\n",
    "    '''input: string or pandas series of text, multiple sentences\n",
    "       returns: float - SMOG grade readability score'''\n",
    "    poly_count = smog_poly_count(text)\n",
    "    sentence_count = count_total_sentences(text)\n",
    "    smog_score = 1.043*np.sqrt(30*poly_count/sentence_count) + 3.1291\n",
    "    return(smog_score)\n",
    "\n",
    "text_df['char_count'] = text_df['text'].apply(count_of_letters)\n",
    "text_df['syl_count'] = text_df['text'].apply(count_total_syllables)\n",
    "text_df['word_count'] = text_df['text'].apply(count_total_words)\n",
    "text_df['char_per_word'] = text_df['char_count']/text_df['word_count']#\n",
    "text_df['syl_per_word'] = text_df['syl_count']/text_df['word_count']#\n",
    "text_df['sent_count'] = text_df['text'].apply(count_total_sentences)\n",
    "text_df['word_per_sent'] = text_df['word_count']/text_df['sent_count']#\n",
    "\n",
    "text_df['dc_word_count'] = text_df['text'].apply(dc_difficult_word_count)\n",
    "text_df['gf_word_count'] = text_df['text'].apply(gf_complex_word_count)\n",
    "text_df['poly_word_count'] = text_df['text'].apply(smog_poly_count)\n",
    "\n",
    "text_df['dc_word_perc'] = text_df['dc_word_count']/text_df['word_count']#\n",
    "text_df['gf_word_perc'] = text_df['gf_word_count']/text_df['word_count']#\n",
    "text_df['poly_word_perc'] = text_df['poly_word_count']/text_df['word_count']#\n",
    "\n",
    "text_df['ari'] = text_df['text'].apply(ari)\n",
    "text_df['flesch_kincaid'] = text_df['text'].apply(flesch_kincaid)\n",
    "text_df['coleman_liau'] = text_df['text'].apply(coleman_liau)\n",
    "text_df['dale_chall'] = text_df['text'].apply(dale_chall)\n",
    "text_df['gunning_fog'] = text_df['text'].apply(gunning_fog)\n",
    "text_df['smog'] = text_df['text'].apply(smog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ca61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------PARALLEL PHRASE COUNT---------')\n",
    "parser = CoreNLPParser()\n",
    "def count_parallels(sents):\n",
    "    count = 0\n",
    "    print_error = True\n",
    "    for phrase in sents:\n",
    "        try:\n",
    "            sent, = parser.parse_text(phrase)\n",
    "        except:\n",
    "            if print_error:\n",
    "                print(\"Be sure Stanford CoreNLP server started and parser instantiated!\")\n",
    "                print(\"Errors also seem to occur with quotes\")\n",
    "                print_error = False\n",
    "            continue\n",
    "        poss = []\n",
    "        words = []\n",
    "        for word in sent.pos():\n",
    "            poss.append(word[1])\n",
    "            words.append(word[0])\n",
    "        stop = False\n",
    "        results = []\n",
    "        for length in range(7,3,-1):\n",
    "            length = min(length, len(words))\n",
    "            for i in range(len(poss)-length+1):\n",
    "                for j in range(len(poss)-length+1):\n",
    "                    if abs(i-j) > length:\n",
    "                        if poss[i:i+length]==poss[j:j+length]:\n",
    "                            if length > 4 or (',' not in poss[i:i+length] and '``' not in poss[i:i+length]):\n",
    "                                results.append([i,j,length])\n",
    "                                count += 1\n",
    "                                stop = True\n",
    "                                break\n",
    "                if stop: break\n",
    "            if stop: break\n",
    "\n",
    "    return(count)\n",
    "\n",
    "#for i in range(len(text_df.text)):\n",
    "#    sent_count, parallel_count = text_df.sentences[3].apply(count_parallels)\n",
    "#    text_df.loc[i, 'sent_count'] = sent_count\n",
    "#    text_df.loc[i, 'parallel_count'] = parallel_count\n",
    "text_df['parallel_count'] = text_df.sentences.apply(count_parallels)\n",
    "text_df['parallel_per_sent'] = text_df.parallel_count/text_df.num_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f514132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------PARSE TREE DEPTH----------')\n",
    "\n",
    "# Make data frame of sentences and parse tree depth of each\n",
    "def walk_tree_depth(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return max(walk_tree_depth(child, depth+1) for child in node.children )\n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "tree_depth = pd.DataFrame(columns = ['date', 'source', 'sentence', 'depth'])\n",
    "for i, speech in enumerate(text_df['sentences']):\n",
    "    for j, sentence in enumerate(speech):\n",
    "        doc = nlp_spacy(sentence)\n",
    "        depth = [walk_tree_depth(sent.root, 0) for sent in doc.sents][0]\n",
    "        tree_depth.loc[len(tree_depth)] = [text_df.index[i], text_df['source'].iloc[i], sentence, depth]\n",
    "\n",
    "mean_depth=tree_depth.groupby(by='date').mean(depth)\n",
    "text_df=pd.merge(text_df, mean_depth, how='left', on='date')\n",
    "tree_depth.to_csv('./Data/genData/sentence_depth_'+suffix+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fd6e4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = text_df.copy()\n",
    "#pos['file'] = pos.index\n",
    "pos.reset_index(drop=False,inplace=True)\n",
    "rest = pd.read_csv('./Data/genData/tidy_data_amrhet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1c4869c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'INTJ',\n",
       " 'num_sents',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'source',\n",
       " 'AUX',\n",
       " 'num_unique_words',\n",
       " 'SCONJ',\n",
       " 'PROPN',\n",
       " 'PART',\n",
       " 'ADV',\n",
       " 'SYM',\n",
       " 'ADP',\n",
       " 'CCONJ',\n",
       " 'ADJ',\n",
       " 'X',\n",
       " 'num_words',\n",
       " 'NUM',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection = list(set(pos.columns).intersection(set(rest.columns)))\n",
    "intersection.remove('file')\n",
    "intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a64e250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest.drop(intersection, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e8115a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(pos, rest, how='inner', on='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e9df4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./Data/genData/tidy_data_amrhet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6beaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
