{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494e4c0e",
   "metadata": {},
   "source": [
    "This notebook scrapes the UC-Santa Barbara presidency site to grab the time line of Obama's presidency, grabs the links, then grabs the speeches. There are about 135 speeches hosted by this site. They also link to some other speeches which are not saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59291453",
   "metadata": {},
   "source": [
    "14 March 2023: Repurposed to get George W Bush speeches from UCSB, the HTML code of which was slightly different than the Obama page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e753b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual suspects...\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time # avoid swamping the americanrhetoric website\n",
    "import os # to check if directory exists and create it if it doesn't\n",
    "from datetime import date, datetime # to parse speech date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69657db4",
   "metadata": {},
   "source": [
    "<A HREF=\"https://www.presidency.ucsb.edu/documents/barack-obama-event-timeline\">UCSB timeline and speech links</A>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e354aa7",
   "metadata": {},
   "source": [
    "<A HREF=\"https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\">Pandas Cheat Sheet</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83df8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ucsburl = \"https://www.presidency.ucsb.edu/documents/barack-obama-event-timeline\"\n",
    "ucsburl = \"https://www.presidency.ucsb.edu/documents/george-w-bush-event-timeline\"\n",
    "ucsbPage = requests.get(ucsburl, headers={'user-agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(ucsbPage.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15235944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obama : <div class=\"WordSection1\">\n",
    "#timeline = soup.find('div', {'class':'WordSection1'})\n",
    "# gwb : <div class=\"field-docs-content\">\n",
    "timeline = soup.find('div', {'class':'field-docs-content'})\n",
    "#timeline = [x for x in timeline.find_all('tr') if str(x).find('https://www.presidency.ucsb.edu/')>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3846bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example event\n",
    "#<tr>\n",
    "#<td style=\"border:none; border-bottom:solid #43b4e0 1.0pt; width:125.05pt; padding:0in 5.4pt 0in 5.4pt\" valign=\"top\" width=\"167\">\n",
    "#  <p align=\"center\" style=\"margin-top:3.0pt; margin-right:0in; margin-bottom:3.0pt; margin-left:0in; text-align:center\">\n",
    "#  <span lang=\"EN-GB\" style=\"font-size:12.0pt\" xml:lang=\"EN-GB\">\n",
    "#    <span style=\"color:black\">1/22/2009</span>\n",
    "#  </span>\n",
    "#  </p>\n",
    "#</td>\n",
    "#<td style=\"border:none; border-bottom:solid #43b4e0 1.0pt; width:299.45pt; padding:0in 5.4pt 0in 5.4pt\" valign=\"top\" width=\"399\">\n",
    "#  <p style=\"margin-top:3.0pt; margin-right:0in; margin-bottom:3.0pt; margin-left:0in\">\n",
    "#    <span style=\"font-size:12.0pt\">\n",
    "#      <a href=\"https://www.presidency.ucsb.edu/documents/executive-order-13491-ensuring-lawful-interrogations\">Executive Order 13491, Ensuring Lawful Interrogations</a>.  Directs that detainees in armed conflict shall be treated humanely and not be subject “to violence to life and person” or “outrages to personal dignity.”\n",
    "#    </span>\n",
    "#  </p>\n",
    "#</td>\n",
    "#</tr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "714c8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 99 99\n"
     ]
    }
   ],
   "source": [
    "# for gwb page\n",
    "dates = []\n",
    "links = []\n",
    "descriptions = []\n",
    "for x in timeline.find_all('tr'):\n",
    "    if str(x).find('https://www.presidency.ucsb.edu/')>0:\n",
    "        date = [da.find('span', {'style': 'color:black'}).text for da in x.find_all('td', {'width': '192'}) \\\n",
    "         if da.find('span', {'style': 'color:black'}) != None]\n",
    "        link = [l.find('a',href=True)['href'] for l in x.find_all('td', {'width': '375'}) if l.find('a') is not None]\n",
    "        desc = [de.text for de in x.find_all('td', {'width': '375'}) if de.find('a') is not None]\n",
    "        dates.append(date)\n",
    "        links.append(link)\n",
    "        descriptions.append(desc)\n",
    "        \n",
    "print(len(dates), len(links), len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ccc44eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Date: <span lang=\"EN-GB\" style=\"font-size:12.0pt\" xml:lang=\"EN-GB\"><span style=\"color:black\">1/22/2009</span></span>\n",
    "# Event: \n",
    "# Width: Obama: 167, gwb: 192\n",
    "dates = [x.find('span', {'style': 'color:black'}).text for x in timeline.find_all('td', {'width': '192'}) \\\n",
    "         if x.find('span', {'style': 'color:black'}) != None and \\\n",
    "         len(x.find('span', {'style': 'color:black'}).text)>4]\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0e54e",
   "metadata": {},
   "source": [
    "<A HREF=\"https://docs.python.org/3/library/datetime.html#datetime.date\">Python datetime documentation</A><BR>\n",
    "<A HREF=\"https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\">Format codes</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d8ccf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11/09/2000']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(dates[2]) = str\n",
    "dates[2]\n",
    "# mm/dd/YYYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66dd0091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 19, 48, 58]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually fix some dates in gwb timeline\n",
    "[i for i, x in enumerate(dates) if len(x)>0 and len(x[0])<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d1e5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates[4] = ['01/22/2001']\n",
    "dates[19] = ['01/29/2002']\n",
    "dates[48] = ['02/02/2005']\n",
    "dates[58] = ['01/31/2006']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e52377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out empty entries\n",
    "import copy # for deep copy\n",
    "newdates = []\n",
    "newlinks = []\n",
    "newdescr = []\n",
    "for i, x in enumerate(dates):\n",
    "    if len(x) > 0:\n",
    "        newdates.append(x[0])\n",
    "        newlinks.append(links[i][0])\n",
    "        newdescr.append(descriptions[i][0])\n",
    "\n",
    "# OK, that looks good, copy back to match the rest of the code\n",
    "dates = copy.deepcopy(newdates)\n",
    "links = copy.deepcopy(newlinks)\n",
    "descriptions = copy.deepcopy(newdescr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5487d9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanDates = []\n",
    "for x in dates:\n",
    "    try:\n",
    "        cleanDates.append(datetime.strptime(x, '%m/%d/%Y').strftime(\"%Y/%m/%d\"))\n",
    "    except:\n",
    "        try:\n",
    "            cleanDates.append(datetime.strptime(x, '0%m/%d/%Y').strftime(\"%Y/%m/%d\"))\n",
    "        except:\n",
    "            try:\n",
    "                cleanDates.append(datetime.strptime(x[0:10], '%m/%d/%Y').strftime(\"%Y/%m/%d\"))\n",
    "            except:\n",
    "                cleanDates.append(datetime.strptime(x[0:10], '%m/%d-%Y').strftime(\"%Y/%m/%d\"))\n",
    "len(cleanDates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "16267171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Event: \n",
    "# Width Obama: 399, gwb: 375\n",
    "#links = [x.find('a',href=True)['href'] for x in timeline.find_all('td', {'width': '375'}) if x.find('a') is not None]\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "99f10c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#font-size:12.0pt\n",
    "# Width Obama: 399, gwb: 375\n",
    "# for the Obama page\n",
    "#descriptions = [x.find('span', {'style': 'font-size:12.0pt'}).text for x in timeline.find_all('td', {'width': '337'}) if x.find('a') is not None]\n",
    "# for the gwb page\n",
    "descriptions = [d.replace('\\xa0', '') for d in descriptions]\n",
    "descriptions = [d.replace('\\n', '') for d in descriptions]\n",
    "descriptions\n",
    "len(descriptions)                  \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b8c9eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=list(zip(cleanDates, links, descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bbbd4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['date', 'link', 'description']\n",
    "mddf = pd.DataFrame(metadata, columns = columns)\n",
    "#df.to_csv('./Data/genData/ucsb_metadata.csv', encoding='utf-8', header=False, index=False)\n",
    "\n",
    "mddf.to_csv('./Data/genData/ucsb_metadata_gwb.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a31dbb",
   "metadata": {},
   "source": [
    "<A HREF=\"https://beautiful-soup-4.readthedocs.io/en/latest/\">Beautiful Soup Documentation</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e3edb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created folder :  GWB\n"
     ]
    }
   ],
   "source": [
    "# Check if Data and associated folders exist for saving speech CSVs to.\n",
    "# https://djangocentral.com/check-if-a-directory-exists-if-not-create-it/\n",
    "\n",
    "#MYDIRS = [\"DataUCSB\"]\n",
    "MYDIRS = [\"GWB\"]\n",
    "\n",
    "# If folder doesn't exist, then create it.\n",
    "for MYDIR in MYDIRS:\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "    if CHECK_FOLDER:\n",
    "        print(MYDIR, \"folder already exists.\")\n",
    "    else:\n",
    "        os.makedirs(MYDIR)\n",
    "        print(\"created folder : \", MYDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9b947002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'text' https://www.presidency.ucsb.edu/statistics/elections/2000 11/07/2000\n",
      "'NoneType' object has no attribute 'text' https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/documents-related-the-2000-election-dispute/1109 11/09/2000\n",
      "'NoneType' object has no attribute 'text' https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/documents-related-the-2000-election-dispute 11/09/2000\n",
      "'NoneType' object has no attribute 'text' https://www.presidency.ucsb.edu/node/332343/ 11/05/2002\n",
      "'NoneType' object has no attribute 'text' https://www.presidency.ucsb.edu/node/332343/ 11/07/2006\n",
      "'NoneType' object has no attribute 'text' https://www.presidency.ucsb.edu/statistics/elections/2008 11/04/2008\n"
     ]
    }
   ],
   "source": [
    "textinfo = []\n",
    "i=0\n",
    "for j, speechlink in enumerate(links):\n",
    "    # check if this is a ucsb presidency link, there are a couple other sites, don't feel like parsing them now.\n",
    "    if 'www.presidency.ucsb.edu' in speechlink:\n",
    "        try:\n",
    "            speechPage = requests.get(speechlink, headers={'user-agent': 'Mozilla/5.0'})\n",
    "        except Exception as e:\n",
    "            print(e, speechlink)\n",
    "            continue # skip this page and go back to beginning of for loop\n",
    "        speechsoup = BeautifulSoup(speechPage.content, 'html.parser')\n",
    "\n",
    "    # speech text is in <div class=\"field-docs-content\">\n",
    "    try:\n",
    "        speechHTML = speechsoup.find('div', {'class':'field-docs-content'})\n",
    "        text = speechHTML.text.replace('\\n', ' ').lstrip().rstrip()\n",
    "        # Each paragraph is in <p> ... </p>, then remove all html tags, including <em>...</em>\n",
    "        speechCSV = [re.sub('\\<.*?\\>', '', str(x)) for x in speechHTML.find_all('p') if speechHTML is not None]\n",
    "        df = pd.DataFrame(speechCSV)\n",
    "        # filename is everything to left of all / in speechlink\n",
    "        #filename = 'DataUCSB/' + re.sub(\"((.*)[\\/])\", \"\", speechlink) + '.csv'\n",
    "        #textfilename = 'DataUCSB/' + re.sub(\"((.*)[\\/])\", \"\", speechlink) + '.txt'\n",
    "        filename = 'GWB/' + re.sub(\"((.*)[\\/])\", \"\", speechlink) + '.csv'\n",
    "        textfilename = 'GWB/' + re.sub(\"((.*)[\\/])\", \"\", speechlink) + '.txt'\n",
    "        df.to_csv(filename, encoding='utf-8', header=False, index=False)\n",
    "        # Write a text file\n",
    "        text_file = open(textfilename, 'w')\n",
    "        text_file.write(text)\n",
    "        text_file.close()\n",
    "        textinfo.append([metadata[i][0], textfilename])\n",
    "    except Exception as e:\n",
    "        print(e, speechlink, dates[j])\n",
    "    time.sleep(5) # trying to be nice here!\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fa319c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dates=['2000/11/07','2000/11/09','2002/11/05','2006/11/07','2008/11/04']\n",
    "mddf=mddf.drop(mddf[mddf['date'].isin(remove_dates)].index).reset_index(drop=True)\n",
    "mddf.to_csv('./Data/genData/ucsb_metadata_gwb.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "15e45eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = pd.DataFrame(textinfo)\n",
    "textdf.to_csv('./Data/genData/speech_and_date_gwb.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c707152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
