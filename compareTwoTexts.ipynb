{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88454644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/genData/text_sentences_words.csv') # this file has the texts with punctuation!\n",
    "# remove [stuff] in between square brackets - should already be gone now\n",
    "def remove_bracket(text):\n",
    "    return re.sub('(\\[[^w]*\\]\\s)', '',text)\n",
    "df['text'] = df['text'].apply(remove_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date1='2012-07-21'\n",
    "source1='oba'\n",
    "text1=df.query('date==@date1 and source==@source1')\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f29675",
   "metadata": {},
   "outputs": [],
   "source": [
    "date2='2010-07-26'\n",
    "source2='wsj'\n",
    "text2=df.query('date==@date2 and source==@source2')\n",
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ea14a",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519705d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full list of processors.... only need tokenize and pos for now\n",
    "#============================\n",
    "#| Processor    | Package   |\n",
    "#----------------------------\n",
    "#| tokenize     | combined  |\n",
    "#| pos          | combined  |\n",
    "#| lemma        | combined  |\n",
    "#| depparse     | combined  |\n",
    "#| sentiment    | sstplus   |\n",
    "#| constituency | wsj       |\n",
    "#| ner          | ontonotes |\n",
    "#============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp_st = stanza.Pipeline('en', processors='tokenize, lemma, pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_count(items):\n",
    "    '''items: list of objects to count\n",
    "       returns: dictionary with key:item, value: cumulative count of each item'''\n",
    "    counts = defaultdict(int)\n",
    "    cumulative_counts = {}\n",
    "    cumulative_count = 0\n",
    "    \n",
    "    for item in items:\n",
    "        counts[item] += 1\n",
    "        cumulative_count += 1\n",
    "        cumulative_counts[item] = cumulative_count\n",
    "        \n",
    "    return(cumulative_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK Lets put it all together and loop over all speeches and count up the POS\n",
    "# The nlp(text) uses a lot of gpu memory, causes errors sometimes, may need to restart notebook to freshen things up\n",
    "pos_dicts = []\n",
    "def get_pos(text):\n",
    "    doc = nlp_st(text) # Run stanza on each speech\n",
    "    mat_of_pos = [[word.pos for word in sentence.words] for sentence in doc.sentences] # matrix of POS for each sentence\n",
    "    # How to flatten a list = [item for sublist in list_of_lists for item in sublist]\n",
    "    list_of_pos = [pos for sentence in mat_of_pos for pos in sentence] # flatten matrix into one list of all pos\n",
    "    pos_counts = cumulative_count(list_of_pos) # get dictionary with key: pos, value: count of that POS in speech\n",
    "    sum1=sum(pos_counts.values())\n",
    "    pos_prop = pos_counts.values()/sum1\n",
    "    print(pos_prop)\n",
    "get_pos(text1.loc[37, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_st(text1.loc[37, 'text']) # Run stanza on each speech\n",
    "mat_of_pos = [[word.pos for word in sentence.words] for sentence in doc.sentences] # matrix of POS for each sentence\n",
    "# How to flatten a list = [item for sublist in list_of_lists for item in sublist]\n",
    "list_of_pos = [pos for sentence in mat_of_pos for pos in sentence] # flatten matrix into one list of all pos\n",
    "pos_counts = cumulative_count(list_of_pos) # get dictionary with key: pos, value: count of that POS in speech\n",
    "sum1=sum(pos_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pick up here later and fix the previous two cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
