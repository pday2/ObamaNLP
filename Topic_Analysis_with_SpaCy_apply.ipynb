{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis with SpaCy & Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS ... so far best: 8, 0.5, False\n",
    "NUM_TOPICS = 8   # number of topics\n",
    "NO_ABOVE = 0.5   # token filter words appearing in more than X% of documents\n",
    "REM_SW = True    # True - removes kaggle stop words, false does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 09:18:37.280811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 09:18:37.435591: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-24 09:18:37.944867: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 09:18:37.944962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 09:18:37.944969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-24 09:18:38.551294: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-24 09:18:38.551314: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-24 09:18:38.551344: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (muddy-HP-ProDesk-600-G3-SFF): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from wordcloud import WordCloud\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "import dateutil.parser as dparser\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook() #Notebook visualisation enabled\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import en_core_web_md\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 743\n"
     ]
    }
   ],
   "source": [
    "# Load up the files\n",
    "#path = './DataUCSB/' # Smaller UCSB dataset\n",
    "#path = './Data/' # larger American Rhetoric dataset\n",
    "paths = ['./Data/', './NYT/', './WSJ/', './GWB/', './speeches/', './Top10/']\n",
    "\n",
    "speeches = []\n",
    "\n",
    "for path in paths:\n",
    "    list_of_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                list_of_files.append(os.path.join(root,file))\n",
    "   \n",
    "    for file in list_of_files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        speeches.append(text)\n",
    "\n",
    "#clean out goofy unicode  space characters \n",
    "speeches = [unicodedata.normalize(\"NFKD\", speech) for speech in speeches if len(speech)>0 ]\n",
    "#clean out xa0 space characters\n",
    "[speech.replace(u'\\xa0', '') for speech in speeches]; # ; supresses output\n",
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "speeches = [remove_bracket(speech) for speech in speeches]\n",
    "# Clean up whitespace\n",
    "speeches = [re.sub(r'[\\s+]', ' ', speech) for speech in speeches]\n",
    "print(\"Total documents:\",len(speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want to remove stop words \n",
    "def rem_stop_words(df):\n",
    "    kaggle_file = open(\"./word_lists/kaggle_stopwords.txt\", \"r\")\n",
    "    kaggle_data = kaggle_file.read()\n",
    "    kaggle_list = [word for word in kaggle_data.split('\\n')]\n",
    "    kaggle_file.close()\n",
    "    my_list = ['thats', 'just', 'im', 'did', 'thing', 'mr', 'al', 'thank', 'okay', 'thank','thanks', \n",
    "               'question', 'joshua', 'president', 'obama', 'Ã¢', u'\\x99s', u'\\x99t', u'\\x99ve', u'\\x99m',u'\\x99re', '\\x99']\n",
    "    stop_list = list(set(kaggle_list) | set(my_list))\n",
    "    stop_words = ENGLISH_STOP_WORDS.union(stop_list)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "            df.iloc[i] = ' '.join([word for word in df.iloc[i].split() if word.lower() not in stop_words])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Tokenization and text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oba_scripts=pd.DataFrame(speeches)\n",
    "oba_scripts = oba_scripts[0]\n",
    "if REM_SW:\n",
    "    oba_scripts=rem_stop_words(oba_scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [governor, family, responder, community, newto...\n",
       "1      [selamat, pagi, wonderful, university, indones...\n",
       "2      [let, collins, introduction, incredible, leade...\n",
       "3      [hello, right, seat, want, becky, patton, extr...\n",
       "4      [morning, great, honor, today, course, truly, ...\n",
       "                             ...                        \n",
       "738    [hello, chicago, doubt, america, place, possib...\n",
       "739    [chairman, dean, great, friend, dick, durbin, ...\n",
       "740    [hello, america, hello, democrats, year, ago, ...\n",
       "741    [majesty, royal, highnesses, distinguished, me...\n",
       "742    [rare, honor, life, follow, hero, john, lewis,...\n",
       "Name: tokens, Length: 743, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load SpaCy English Model\n",
    "nlp = en_core_web_md.load()\n",
    "#Tags to remove\n",
    "extags = ['PRON','CCONJ','PUNCT','PART','DET','ADP','NUM','SYM','SPACE']\n",
    "docs = oba_scripts.apply(nlp)\n",
    "tokens=[]\n",
    "#SpaCy tokenization + lemmatization + lowercase\n",
    "for speech in docs:\n",
    "    scr_tok = [token.lemma_.lower() for token in speech if token.pos_ not in extags and not token.is_stop and token.is_alpha]\n",
    "    tokens.append(scr_tok)\n",
    "data = pd.DataFrame()\n",
    "data['tokens'] = tokens\n",
    "data['tokens']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Analysis model and coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = Dictionary(data['tokens'])\n",
    "#Filter out tokens that appear in less than 5 speeches, and tokens that appear in more than 70% of speeches since they are too general. Keep the top 1000 most frequent tokens\n",
    "token_dict.filter_extremes(no_below=5,no_above=NO_ABOVE,keep_n=1000)\n",
    "\n",
    "#Convert token counts into bag of words (BoW) corpus\n",
    "corpus = [token_dict.doc2bow(speech) for speech in data['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Construct and train unsupervised LDA model + Determine optimal number of topics\\numtopics, umscore = [], []\\ncvtopics, cvscore = [], []\\n#Compute coherence score using C_umass:\\nfor i in range(3,15,1):\\n    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=50, num_topics=i, workers=4, passes=10, random_state=47)\\n    #LdamultiCore uses multiple cores to speed up model training, use with caution if you have a weaker PC! (Find your max number of cores with ctrl+shift+esc, under CPU)\\n    cm = CoherenceModel(model=model, corpus=corpus, dictionary=token_dict, coherence='u_mass')\\n\\n    umtopics.append(i)\\n    umscore.append(cm.get_coherence())\\n#Compute coherence score using C_v: \\nfor i in range (3,15,1):\\n    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=10, num_topics=i, workers = 4, passes=10, random_state=47)\\n    cm = CoherenceModel(model=model, texts = data['tokens'], corpus=corpus, dictionary=token_dict, coherence='c_v')\\n\\n    cvtopics.append(i)\\n    cvscore.append(cm.get_coherence())\\n#The difference in coherence score measures is the method in which the text is segmented and probability is calculated\\n#Adjustable threshold for visualising with red vertical lines\\nthreshold=9\\nfig, (ax1, ax2) = plt.subplots(1,2)\\nfig.suptitle('Coherence score by topic count using C_umass and C_v measure')\\nfig.subplots_adjust(wspace=0.4)\\n\\nax1.plot(umtopics,umscore)\\nax1.set_xlabel('Number of Topics')\\nax1.set_ylabel('Coherence Score (C_umass)')\\nax1.axvline(x=threshold,c='red')\\n\\nax2.plot(cvtopics,cvscore)\\nax2.set_xlabel('Number of Topics')\\nax2.set_ylabel('Coherence Score (C_v)')\\nax2.axvline(x=threshold,c='red')\\n#For both scores, higher values are better. Choice of topic count is subjective, but both scores must be taken into account.\\nplt.show()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't need to run this everytime, commented out to save time\n",
    "'''\n",
    "#Construct and train unsupervised LDA model + Determine optimal number of topics\n",
    "umtopics, umscore = [], []\n",
    "cvtopics, cvscore = [], []\n",
    "#Compute coherence score using C_umass:\n",
    "for i in range(3,15,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=50, num_topics=i, workers=4, passes=10, random_state=47)\n",
    "    #LdamultiCore uses multiple cores to speed up model training, use with caution if you have a weaker PC! (Find your max number of cores with ctrl+shift+esc, under CPU)\n",
    "    cm = CoherenceModel(model=model, corpus=corpus, dictionary=token_dict, coherence='u_mass')\n",
    "\n",
    "    umtopics.append(i)\n",
    "    umscore.append(cm.get_coherence())\n",
    "#Compute coherence score using C_v: \n",
    "for i in range (3,15,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=10, num_topics=i, workers = 4, passes=10, random_state=47)\n",
    "    cm = CoherenceModel(model=model, texts = data['tokens'], corpus=corpus, dictionary=token_dict, coherence='c_v')\n",
    "\n",
    "    cvtopics.append(i)\n",
    "    cvscore.append(cm.get_coherence())\n",
    "#The difference in coherence score measures is the method in which the text is segmented and probability is calculated\n",
    "#Adjustable threshold for visualising with red vertical lines\n",
    "threshold=9\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.suptitle('Coherence score by topic count using C_umass and C_v measure')\n",
    "fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "ax1.plot(umtopics,umscore)\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Coherence Score (C_umass)')\n",
    "ax1.axvline(x=threshold,c='red')\n",
    "\n",
    "ax2.plot(cvtopics,cvscore)\n",
    "ax2.set_xlabel('Number of Topics')\n",
    "ax2.set_ylabel('Coherence Score (C_v)')\n",
    "ax2.axvline(x=threshold,c='red')\n",
    "#For both scores, higher values are better. Choice of topic count is subjective, but both scores must be taken into account.\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\num = pd.DataFrame({\\'number of topics\\': umtopics, \\'score\\':umscore})\\num[\\'method\\'] = \\'umass\\'\\ncv = pd.DataFrame({\\'number of topics\\': cvtopics, \\'score\\':cvscore})\\ncv[\\'method\\'] = \\'cv\\'\\ncoherence = pd.concat([um,cv])\\n\\nfig = px.line(coherence, x=\"number of topics\", y=\"score\", facet_row=\"method\", width=600, \\n              title=\\'Coherence score by topic count using C_umass and C_v measure\\')\\nfig.update_yaxes(matches=None)\\nfig.show()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "um = pd.DataFrame({'number of topics': umtopics, 'score':umscore})\n",
    "um['method'] = 'umass'\n",
    "cv = pd.DataFrame({'number of topics': cvtopics, 'score':cvscore})\n",
    "cv['method'] = 'cv'\n",
    "coherence = pd.concat([um,cv])\n",
    "\n",
    "fig = px.line(coherence, x=\"number of topics\", y=\"score\", facet_row=\"method\", width=600, \n",
    "              title='Coherence score by topic count using C_umass and C_v measure')\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig.write_image(\"./plots/coherence_score_by_topic_num.png\", format='png', engine='kaleido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the coherence scores, best is usually the max. In this case we choose to select 6 topics, since it is the max value for the C_v score while also having a relatively higher C_umass score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"energy\" + 0.017*\"business\" + 0.015*\"company\" + 0.011*\"oil\" + 0.011*\"economic\" + 0.010*\"financial\" + 0.009*\"crisis\" + 0.009*\"clean\" + 0.009*\"industry\" + 0.008*\"plan\"'),\n",
       " (1,\n",
       "  '0.016*\"peace\" + 0.015*\"freedom\" + 0.012*\"democracy\" + 0.011*\"free\" + 0.010*\"human\" + 0.010*\"citizen\" + 0.009*\"europe\" + 0.008*\"generation\" + 0.007*\"century\" + 0.007*\"common\"'),\n",
       " (2,\n",
       "  '0.014*\"progress\" + 0.012*\"africa\" + 0.012*\"partner\" + 0.012*\"region\" + 0.010*\"human\" + 0.010*\"global\" + 0.010*\"partnership\" + 0.009*\"asia\" + 0.008*\"democracy\" + 0.008*\"trade\"'),\n",
       " (3,\n",
       "  '0.017*\"gun\" + 0.016*\"intelligence\" + 0.012*\"protect\" + 0.012*\"national\" + 0.010*\"court\" + 0.009*\"enforcement\" + 0.009*\"attack\" + 0.008*\"public\" + 0.008*\"terrorist\" + 0.008*\"review\"'),\n",
       " (4,\n",
       "  '0.022*\"health\" + 0.021*\"care\" + 0.015*\"tax\" + 0.015*\"pay\" + 0.013*\"insurance\" + 0.013*\"business\" + 0.011*\"reform\" + 0.010*\"cut\" + 0.010*\"college\" + 0.009*\"education\"'),\n",
       " (5,\n",
       "  '0.031*\"iraq\" + 0.023*\"military\" + 0.020*\"terrorist\" + 0.017*\"afghanistan\" + 0.016*\"troop\" + 0.012*\"attack\" + 0.010*\"iraqi\" + 0.010*\"serve\" + 0.010*\"qaeda\" + 0.009*\"ally\"'),\n",
       " (6,\n",
       "  '0.042*\"nuclear\" + 0.039*\"iran\" + 0.023*\"israel\" + 0.022*\"weapon\" + 0.017*\"international\" + 0.017*\"deal\" + 0.012*\"sanction\" + 0.011*\"program\" + 0.010*\"official\" + 0.010*\"obama\"'),\n",
       " (7,\n",
       "  '0.014*\"love\" + 0.011*\"god\" + 0.008*\"school\" + 0.007*\"white\" + 0.007*\"story\" + 0.007*\"talk\" + 0.006*\"black\" + 0.006*\"faith\" + 0.006*\"feel\" + 0.006*\"old\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct final model with NUM_TOPICS topics (Increase iterations and passes because it is the final model)\n",
    "finalmodel = LdaMulticore(corpus=corpus, \n",
    "                          id2word=token_dict, \n",
    "                          iterations=100, \n",
    "                          num_topics=NUM_TOPICS, \n",
    "                          workers = 4, \n",
    "                          passes=100, \n",
    "                          random_state=47)\n",
    "\n",
    "finalmodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct final model with NUM_TOPICS topics (Increase iterations and passes because it is the final model)\n",
    "finalmodel_8 = LdaMulticore(corpus=corpus, \n",
    "                          id2word=token_dict, \n",
    "                          iterations=100, \n",
    "                          num_topics=8, \n",
    "                          workers = 4, \n",
    "                          passes=100, \n",
    "                          random_state=47)\n",
    "\n",
    "finalmodel_8.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=pd.DataFrame(finalmodel_8.print_topics(),columns=['del', 'topic'])\n",
    "save_model.drop('del', axis=1, inplace=True)\n",
    "#save_model.to_csv('./topics/topics_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmodel_8.get_topics() # shape: num_topics x vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[topic for num, topic in finalmodel_8.show_topics(num_topics=8, num_words=8,formatted=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=finalmodel_8.show_topics(num_topics=8, num_words=8,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "#Below Code Prints Topics and Words\n",
    "for topic,words in topics_words:\n",
    "    print(str(topic)+ \"::\"+ str(words))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for first speech - DNC keynote speech 2004\n",
    "finalmodel_8[corpus][0]\n",
    "#Main topic is Topic 3, which seems to be some sort of mixed bag. The speech itself covers his personal life and the democratic party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is an advanced visualisation of the topics. Each circle represents a topic and upon hovering over a circle, the bars on the right illustrate the frequency of words that appear in the topic. Closer circles are more related, which is logical since political arguments correlate highly with voting campaigns. Visualisation of circles is done through PCA dimension reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The topic numbers and order in the plot below do NOT correspond to the topic numbers in print_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "lda_display = pyLDAvis.gensim_models.prepare(topic_model=finalmodel_8, corpus=corpus, dictionary=token_dict)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(dict(token_dict).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num topics x vocab size, here 9x1000\n",
    "topic_vals = pd.DataFrame(finalmodel_8.get_topics(), columns=columns)\n",
    "topic_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_by_doc=list(finalmodel_8[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[doc][topic][0-id] or [1-value]\n",
    "doc_topics = []\n",
    "for doc_num in range(len(topic_by_doc)):\n",
    "    topics_for_doc = [0] * len(finalmodel_8.print_topics())\n",
    "    for i in range(len(finalmodel_8.print_topics())):\n",
    "        if (i < len(topic_by_doc[doc_num])):\n",
    "            topics_for_doc[topic_by_doc[doc_num][i][0]]=topic_by_doc[doc_num][i][1]\n",
    "    doc_topics.append(topics_for_doc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_topics is num_docs x num_topics or 743 rows x 9 cols\n",
    "doc_topics = pd.DataFrame(doc_topics)\n",
    "doc_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_words = doc_topics.dot(topic_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum for each document should by 1-ish\n",
    "docs_words.sum(axis=1) # 0-rows, 1-cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load just the 101 speeches used for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the files\n",
    "\n",
    "paths = ['./speeches/']\n",
    "\n",
    "speeches = []\n",
    "\n",
    "for path in paths:\n",
    "    list_of_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                list_of_files.append(os.path.join(root,file))\n",
    "   \n",
    "    for file in list_of_files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        speeches.append(text)\n",
    "\n",
    "#clean out goofy unicode  space characters \n",
    "speeches = [unicodedata.normalize(\"NFKD\", speech) for speech in speeches if len(speech)>0 ]\n",
    "#clean out xa0 space characters\n",
    "[speech.replace(u'\\xa0', '') for speech in speeches]; # ; supresses output\n",
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "speeches = [remove_bracket(speech) for speech in speeches]\n",
    "# Clean up whitespace\n",
    "speeches = [re.sub(r'[\\s+]', ' ', speech) for speech in speeches]\n",
    "\n",
    "date_text = [file[11:21] for file in list_of_files]\n",
    "\n",
    "df = pd.DataFrame({'date' : date_text,\n",
    "                   'file' : list_of_files,\n",
    "                   'text' : speeches})\n",
    "df.date = pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "speeches101 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 400+ American Rhetoric speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the files\n",
    "#path = './DataUCSB/' # Smaller UCSB dataset\n",
    "#path = './Data/' # larger American Rhetoric dataset\n",
    "paths = ['./Data/']\n",
    "\n",
    "speeches = []\n",
    "\n",
    "for path in paths:\n",
    "    list_of_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                list_of_files.append(os.path.join(root,file))\n",
    "   \n",
    "    for file in list_of_files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        f.close()\n",
    "        speeches.append(text)\n",
    "\n",
    "#clean out goofy unicode  space characters \n",
    "speeches = [unicodedata.normalize(\"NFKD\", speech) for speech in speeches if len(speech)>0 ]\n",
    "#clean out xa0 space characters\n",
    "[speech.replace(u'\\xa0', '') for speech in speeches]; # ; supresses output\n",
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "speeches = [remove_bracket(speech) for speech in speeches]\n",
    "# Clean up whitespace\n",
    "speeches = [re.sub(r'[\\s+]', ' ', speech) for speech in speeches]\n",
    "\n",
    "df = pd.DataFrame({'file' : list_of_files,\n",
    "                   'text' : speeches})\n",
    "datetitle = pd.read_csv('datetitle.csv')\n",
    "#datetitle.url = [file.replace('Data/', './Data/') for file in datetitle.url]\n",
    "datetitle.date = pd.to_datetime(datetitle.date, format='%Y-%m-%d')\n",
    "datetitle = datetitle.drop('title', axis=1)\n",
    "datetitle = datetitle.rename(columns={'url': 'file'})\n",
    "df = pd.merge(df, datetitle, how='inner', on='file')\n",
    "df = df.sort_values(by='date', ignore_index=True)\n",
    "amrhet = df[['date', 'file', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()\n",
    "def prepare_text(text_list):\n",
    "    #Tags to remove\n",
    "    extags = ['PRON','CCONJ','PUNCT','PART','DET','ADP','NUM','SYM','SPACE']\n",
    "    docs = text_list.apply(nlp)\n",
    "    tokens=[]\n",
    "    #SpaCy tokenization + lemmatization + lowercase\n",
    "    for speech in docs:\n",
    "        scr_tok = [token.lemma_.lower() for token in speech if token.pos_ not in extags and not token.is_stop and token.is_alpha]\n",
    "        tokens.append(scr_tok)\n",
    "    data = pd.DataFrame()\n",
    "    data['tokens'] = tokens\n",
    "\n",
    "    token_dict = Dictionary(data['tokens'])\n",
    "    #Filter out tokens that appear in less than 5 speeches, and tokens that appear in more than 70% of speeches since they are too general. Keep the top 1000 most frequent tokens\n",
    "    token_dict.filter_extremes(no_below=5,no_above=NO_ABOVE,keep_n=1000)\n",
    "\n",
    "    #Convert token counts into bag of words (BoW) corpus\n",
    "    corpus = [token_dict.doc2bow(speech) for speech in data['tokens']]\n",
    "    \n",
    "    return([token_dict, corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REM_SW:\n",
    "    amrhet.text=rem_stop_words(amrhet.text)\n",
    "    speeches101.text=rem_stop_words(speeches101.text)\n",
    "amrhet_token_dict, amrhet_corpus = prepare_text(amrhet.text)\n",
    "speeches_token_dict, speeches_corpus = prepare_text(speeches101.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amrhet['topics'] = [finalmodel_8[corpus] for corpus in amrhet_corpus]\n",
    "speeches101['topics'] = [finalmodel_8[corpus] for corpus in speeches_corpus]\n",
    "amrhet_topics = [finalmodel_8[corpus] for corpus in amrhet_corpus]\n",
    "speeches101_topics = [finalmodel_8[corpus] for corpus in speeches_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert topic list to matrix of values to put in a data frame\n",
    "def topic_to_matrix(topic_by_doc):\n",
    "    doc_topics = []\n",
    "    for doc_num in range(len(topic_by_doc)):\n",
    "        topics_for_doc = [0] * len(finalmodel_8.print_topics())\n",
    "        for i in range(len(finalmodel_8.print_topics())):\n",
    "            if (i < len(topic_by_doc[doc_num])):\n",
    "                topics_for_doc[topic_by_doc[doc_num][i][0]]=topic_by_doc[doc_num][i][1]\n",
    "        doc_topics.append(topics_for_doc);\n",
    "    return(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amrhet_topic_mat = pd.DataFrame(topic_to_matrix(amrhet_topics))\n",
    "speeches101_topic_mat = pd.DataFrame(topic_to_matrix(speeches101_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amrhet = pd.concat([amrhet,amrhet_topic_mat], axis=1)\n",
    "speeches101 = pd.concat([speeches101,speeches101_topic_mat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave commented as to not accidently overwrite something important\n",
    "#amrhet.to_csv('topics_amrhet_oba.csv', index=False)\n",
    "#speeches101.to_csv('topics_speeches_oba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy = pd.read_csv('tidy_data_oba.csv')\n",
    "tidy.date = pd.to_datetime(tidy.date, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = speeches101.drop(['file', 'text', 'topics'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.read_csv('./topics/topics_8.csv')\n",
    "topic_lst = list(topics.topic_name)\n",
    "col_lst = ['date']\n",
    "for topic in topic_lst: col_lst.append(topic)\n",
    "ar.columns = col_lst\n",
    "topic_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_topic = pd.merge(tidy, ar, how='left', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy_topic.to_csv('tidy_topic_oba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = tidy_topic.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_corr = correlation.loc['ADJ':'chars_per_sent_std', 'economy':'civil rights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_corr.columns = list(topics.topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(x):\n",
    "    return ['font-weight: bold' if abs(v) > 0.30 else ''\n",
    "                for v in x]\n",
    "\n",
    "topic_corr.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(tidy_topic, x=\"intl relations\", y=\"NUM\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(tidy_topic, x=\"middle east\", y=\"syl_per_word\",\n",
    "                title=\"Topic: Middle East vs Syllables per word\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"./plots/middle_east_vs_syllables_per_word.png\", format='png', engine='kaleido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(tidy_topic, x=\"middle east\", y=\"smog\",\n",
    "                title=\"Topic: Middle East vs SMOG readbility index\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"./plots/middle_east_vs_SMOG.png\", format='png', engine='kaleido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(tidy_topic, x=\"intl relations\", y=\"anger\",\n",
    "                 labels = {'intl relations':'international relations'},\n",
    "                title=\"International relations vs anger\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"./plots/scatter_internationalrelations_vs_anger.png\", format='png', engine='kaleido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "amrhet.text = amrhet.text.apply(remove_bracket)\n",
    "def get_encodings(text):\n",
    "    return list(nlp(text).vector)\n",
    "amrhet['enc'] = amrhet.text.apply(get_encodings)\n",
    "pca_data = pd.DataFrame(amrhet['enc'].to_list(), index=amrhet.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_data.to_csv('amrhet_spacy_encodings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove [stuff] in between square brackets\n",
    "def remove_bracket(text):\n",
    "    return re.sub(r'(\\[[^w]*\\]\\s)', '',text)\n",
    "speeches101.text = speeches101.text.apply(remove_bracket)\n",
    "def get_encodings(text):\n",
    "    return list(nlp(text).vector)\n",
    "speeches101['enc'] = speeches101.text.apply(get_encodings)\n",
    "pca_data2 = pd.DataFrame(speeches101['enc'].to_list(), index=speeches101.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_data2.to_csv('speeches_spacy_encodings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence revisisted - investigate all four methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct and train unsupervised LDA model + Determine optimal number of topics\n",
    "umtopics, umscore = [], []\n",
    "cvtopics, cvscore = [], []\n",
    "ucitopics, uciscore = [], []\n",
    "npmitopics, npmiscore = [], []\n",
    "#Compute coherence score using C_umass:\n",
    "for i in range(3,15,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=50, num_topics=i, workers=4, passes=10, random_state=47)\n",
    "    #LdamultiCore uses multiple cores to speed up model training, use with caution if you have a weaker PC! (Find your max number of cores with ctrl+shift+esc, under CPU)\n",
    "    cm = CoherenceModel(model=model, corpus=corpus, dictionary=token_dict, coherence='u_mass')\n",
    "\n",
    "    umtopics.append(i)\n",
    "    umscore.append(cm.get_coherence())\n",
    "#Compute coherence score using C_v: \n",
    "for i in range (3,15,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=10, num_topics=i, workers = 4, passes=10, random_state=47)\n",
    "    cm = CoherenceModel(model=model, texts = data['tokens'], corpus=corpus, dictionary=token_dict, coherence='c_v')\n",
    "\n",
    "    cvtopics.append(i)\n",
    "    cvscore.append(cm.get_coherence())\n",
    "#The difference in coherence score measures is the method in which the text is segmented and probability is calculated\n",
    "\n",
    "#Compute coherence score using C_uci: \n",
    "for i in range (3,15,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=10, num_topics=i, workers = 4, passes=10, random_state=47)\n",
    "    cm = CoherenceModel(model=model, texts = data['tokens'], corpus=corpus, dictionary=token_dict, coherence='c_uci')\n",
    "\n",
    "    ucitopics.append(i)\n",
    "    uciscore.append(cm.get_coherence())\n",
    "    \n",
    "#Compute coherence score using C_npmi: \n",
    "for i in range (3,15,1):\n",
    "    model = LdaMulticore(corpus=corpus, id2word=token_dict, iterations=10, num_topics=i, workers = 4, passes=10, random_state=47)\n",
    "    cm = CoherenceModel(model=model, texts = data['tokens'], corpus=corpus, dictionary=token_dict, coherence='c_npmi')\n",
    "\n",
    "    npmitopics.append(i)\n",
    "    npmiscore.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjustable threshold for visualising with red vertical lines\n",
    "threshold=9\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4)\n",
    "fig.suptitle('Coherence score by topic count using C_umass and C_v measure')\n",
    "fig.subplots_adjust(wspace=0.4)\n",
    "\n",
    "ax1.plot(umtopics,umscore)\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Coherence Score (C_umass)')\n",
    "ax1.axvline(x=threshold,c='red')\n",
    "\n",
    "ax2.plot(cvtopics,cvscore)\n",
    "ax2.set_xlabel('Number of Topics')\n",
    "ax2.set_ylabel('Coherence Score (C_v)')\n",
    "ax2.axvline(x=threshold,c='red')\n",
    "\n",
    "ax3.plot(ucitopics,uciscore)\n",
    "ax3.set_xlabel('Number of Topics')\n",
    "ax3.set_ylabel('Coherence Score (C_uci)')\n",
    "ax3.axvline(x=threshold,c='red')\n",
    "\n",
    "ax4.plot(npmitopics,npmiscore)\n",
    "ax4.set_xlabel('Number of Topics')\n",
    "ax4.set_ylabel('Coherence Score (C_npmi)')\n",
    "ax4.axvline(x=threshold,c='red')\n",
    "\n",
    "#For both scores, higher values are better. Choice of topic count is subjective, but both scores must be taken into account.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um = pd.DataFrame({'number of topics': umtopics, 'score':umscore})\n",
    "um['method'] = 'umass'\n",
    "cv = pd.DataFrame({'number of topics': cvtopics, 'score':cvscore})\n",
    "cv['method'] = 'cv'\n",
    "uci = pd.DataFrame({'number of topics': ucitopics, 'score':uciscore})\n",
    "uci['method'] = 'uci'\n",
    "npmi = pd.DataFrame({'number of topics': npmitopics, 'score':npmiscore})\n",
    "npmi['method'] = 'npmi'\n",
    "coherence = pd.concat([um,cv,uci,npmi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(coherence, x=\"number of topics\", y=\"score\", facet_row=\"method\", width=600, \n",
    "              title='Coherence score by topic count using 4 methods')\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"./plots/coherence_scores_4_methods.png\", format='png', engine='kaleido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence['rank']=coherence.groupby(by='method').rank(axis=0,ascending=False).score\n",
    "rank = coherence.groupby('number of topics').sum('rank')\n",
    "rank['number of topics'] = rank.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eight topics returns the best rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(rank, x=\"number of topics\", y=\"rank\", width=600, \n",
    "             labels={'rank':'sum of ranks'},\n",
    "             title='Sum of coherence ranks for the four methods')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"./plots/coherence_score_rank.png\", format='png', engine='kaleido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmodel_8.show_topics(num_topics=8, num_words=20,formatted=True)[0] # economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmodel_8.show_topics(num_topics=8, num_words=20,formatted=True)[7] # civil rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "107f2f6f19e96c840132efb8a7e6b8d8e9855354386e3f52164003e5deff7bc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
